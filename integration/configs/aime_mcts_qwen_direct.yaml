device: "cuda"

# MCTS + Qwen直接生成代码配置
# MCTS tree search with Qwen direct code generation
#
# 对齐原版AFlow + VERL，不简化训练流程，无超出框架创新

total_epochs: 10
episodes_per_epoch: 6
update_frequency: 1
eval_episodes: 2
save_frequency: 1

output_dir: "/content/drive/MyDrive/agentflow/outputs/mcts_qwen"
experience_pool_size: 500

environment:
  train_datasets:
    - "AIME"
  test_datasets:
    - "AIME"
  data_path: "/content/agentflow/AFlow/data/AIME_2024.jsonl"
  train_test_split: 0.8

  # 优化LLM配置（仅用于fallback，主要使用Qwen）
  opt_llm_config:
    model: "gpt-4o-mini"
    key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    temperature: 0.9

  exec_llm_config:
    model: "gpt-4o-mini"
    key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    temperature: 0.7

  operators:
    - "Custom"
    - "ScEnsemble"
    - "Test"
    - "Review"
    - "Revise"

  env_num: 1
  group_n: 1
  max_rounds: 5
  validation_rounds: 3
  sample: 24

  # ✨ MCTS + Qwen直接生成核心配置
  use_dynamic_optimizer: true  # 启用MCTS树搜索
  rl_weight: 0.5  # UCB + Q-value融合权重

  # ✨ 启用Qwen直接生成代码（替代GPT-4）
  use_qwen_code_generation: true  # 让Qwen直接生成完整Python代码
  qwen_max_retries: 2  # Qwen语法错误时的最大重试次数

  workflow_sample_count: 15

rl:
  policy:
    model_path: "/root/models/Qwen2.5-7B-Instruct"
    use_lora: true
    lora_r: 16
    lora_alpha: 32

  # GRPO训练参数
  learning_rate: 0.00003
  batch_size: 4
  ppo_epochs: 4  # PPO epochs用于GRPO的多轮优化
  ppo_clip: 0.2
  gamma: 0.99  # 保留用于兼容性（GRPO不使用discount）
  entropy_coef: 0.1  # 提高探索性，防止operator使用退化
  gradient_clip: 1.0

logging:
  level: "INFO"

debug:
  verbose_env: true
