#!/usr/bin/env python3
"""
ä¼˜åŒ–çš„è®­ç»ƒè„šæœ¬ - Optimized Training Script
åŒ…å«è¯¾ç¨‹å­¦ä¹ ã€å¥–åŠ±å¡‘å½¢ã€æ”¹è¿›çš„æ£€æŸ¥ç‚¹ä¿å­˜ç­‰åŠŸèƒ½
"""

import os
import sys
import argparse
import yaml
import time
import json
from typing import Dict, Any, List
from pathlib import Path
import torch
import numpy as np

# Add paths
AFLOW_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
VERL_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'verl-agent'))

sys.path.insert(0, AFLOW_PATH)
sys.path.insert(0, VERL_PATH)
sys.path.insert(0, os.path.dirname(__file__))

# Import components
try:
    from scripts.shared_experience import SharedExperiencePool
    from scripts.logs import logger

    from unified_state import StateManager
    from trainable_qwen_policy import TrainableQwenPolicy
    from rl_trainer import RLTrainer
    from deep_workflow_env_with_meta import create_deep_workflow_env  # ä½¿ç”¨å…ƒå­¦ä¹ å¢å¼ºç‰ˆæœ¬
    from workflow_prompt_manager import get_prompt_manager

    IMPORTS_AVAILABLE = True
except ImportError as e:
    print(f"Warning: Could not import required components: {e}")
    import traceback
    traceback.print_exc()
    IMPORTS_AVAILABLE = False


class RewardShaper:
    """å¥–åŠ±å¡‘å½¢å™¨ - è§£å†³ç¨€ç–å¥–åŠ±é—®é¢˜"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.enabled = config.get('enable', True)
        self.partial_credit = config.get('partial_credit', True)
        self.improvement_bonus = config.get('improvement_bonus', 0.1)
        self.proximity_reward = config.get('proximity_reward', True)
        self.proximity_scale = config.get('proximity_scale', 0.5)
        self.baseline_reward = config.get('baseline_reward', 0.01)

        self.history = {}  # è®°å½•æ¯ä¸ªé—®é¢˜çš„å†å²å¾—åˆ†

    def shape_reward(self,
                    raw_reward: float,
                    expected_answer: Any,
                    actual_answer: Any,
                    problem_id: str) -> float:
        """
        å¯¹åŸå§‹å¥–åŠ±è¿›è¡Œå¡‘å½¢

        Args:
            raw_reward: åŸå§‹å¥–åŠ±ï¼ˆ0æˆ–1ï¼‰
            expected_answer: æœŸæœ›ç­”æ¡ˆ
            actual_answer: å®é™…ç­”æ¡ˆ
            problem_id: é—®é¢˜ID

        Returns:
            å¡‘å½¢åçš„å¥–åŠ±
        """
        if not self.enabled:
            return raw_reward

        shaped_reward = raw_reward

        # 1. åŸºç¡€å¥–åŠ±ï¼šé¿å…å…¨0
        if raw_reward == 0:
            shaped_reward += self.baseline_reward

        # 2. æ¥è¿‘åº¦å¥–åŠ±ï¼šç­”æ¡ˆæ¥è¿‘æ­£ç¡®ç­”æ¡ˆä¹Ÿç»™å¥–åŠ±
        if self.proximity_reward and raw_reward == 0:
            try:
                expected_num = int(expected_answer)
                actual_num = int(actual_answer) if actual_answer else 0

                # è®¡ç®—ç›¸å¯¹è¯¯å·®
                if expected_num != 0:
                    relative_error = abs(expected_num - actual_num) / abs(expected_num)
                    # è¯¯å·®è¶Šå°ï¼Œå¥–åŠ±è¶Šé«˜
                    proximity = max(0, 1 - relative_error)
                    shaped_reward += proximity * self.proximity_scale
            except (ValueError, TypeError):
                pass

        # 3. è¿›æ­¥å¥–åŠ±ï¼šæ¯”ä¸Šä¸€æ¬¡å¥½å°±ç»™å¥–åŠ±
        if problem_id in self.history:
            last_reward = self.history[problem_id]
            if shaped_reward > last_reward:
                shaped_reward += self.improvement_bonus

        self.history[problem_id] = shaped_reward

        return shaped_reward


class CurriculumScheduler:
    """è¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.enabled = config.get('enable', False)
        self.stages = config.get('stages', [])

    def get_sample_size(self, epoch: int) -> int:
        """æ ¹æ®å½“å‰epochè¿”å›åº”è¯¥ä½¿ç”¨çš„æ ·æœ¬æ•°é‡"""
        if not self.enabled:
            return 12  # é»˜è®¤å…¨éƒ¨

        for stage in self.stages:
            epoch_range = stage['epoch']
            # æ”¯æŒä¸¤ç§æ ¼å¼: æ•´æ•° (1, 2, 3) æˆ– å­—ç¬¦ä¸²èŒƒå›´ ("1-5", "6-10")
            if isinstance(epoch_range, int):
                if epoch == epoch_range:
                    return stage['sample_size']
            elif isinstance(epoch_range, str) and '-' in epoch_range:
                start, end = map(int, epoch_range.split('-'))
                if start <= epoch <= end:
                    return stage['sample_size']

        return 12  # é»˜è®¤


class OptimizedTrainer:
    """ä¼˜åŒ–çš„è®­ç»ƒå™¨"""

    def __init__(self, config: Dict[str, Any]):
        """åˆå§‹åŒ–"""
        if not IMPORTS_AVAILABLE:
            raise RuntimeError("Required imports not available")

        self.config = config
        self.device = torch.device(config.get('device', 'cuda' if torch.cuda.is_available() else 'cpu'))

        print("=" * 80)
        print("  ä¼˜åŒ–çš„AIMEè®­ç»ƒç³»ç»Ÿ - Optimized AIME Training System")
        print("=" * 80)
        print(f"\nDevice: {self.device}")
        print(f"PyTorch: {torch.__version__}")
        print(f"CUDA: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")

        # è®­ç»ƒå‚æ•°
        self.total_epochs = config.get('total_epochs', 20)
        self.episodes_per_epoch = config.get('episodes_per_epoch', 8)
        self.update_frequency = config.get('update_frequency', 2)
        self.save_frequency = config.get('save_frequency', 1)

        # è·¯å¾„
        self.output_dir = Path(config.get('output_dir', './output/optimized'))
        self.output_dir.mkdir(parents=True, exist_ok=True)

        self.checkpoint_dir = self.output_dir / 'checkpoints'
        self.checkpoint_dir.mkdir(exist_ok=True)

        # è¯¾ç¨‹å­¦ä¹ 
        self.curriculum = CurriculumScheduler(config.get('curriculum', {}))
        logger.info(f"âœ“ è¯¾ç¨‹å­¦ä¹ : {'å¯ç”¨' if self.curriculum.enabled else 'ç¦ç”¨'}")

        # å¥–åŠ±å¡‘å½¢
        reward_config = config.get('rl', {}).get('reward_shaping', {})
        self.reward_shaper = RewardShaper(reward_config)
        logger.info(f"âœ“ å¥–åŠ±å¡‘å½¢: {'å¯ç”¨' if self.reward_shaper.enabled else 'ç¦ç”¨'}")

        # å…±äº«ç»„ä»¶
        self.shared_experience_pool = SharedExperiencePool(
            max_size=config.get('experience_pool_size', 5000),
            eviction_strategy='lowest_score'
        )
        self.state_manager = StateManager()
        self.prompt_manager = get_prompt_manager()

        # ç¯å¢ƒé…ç½®
        self.env_config = config.get('environment', {})
        self.train_datasets = self.env_config.get('train_datasets', ['AIME'])

        # RLé…ç½®
        self.rl_config = config.get('rl', {})

        # åŠ è½½ç­–ç•¥
        print("\n" + "=" * 80)
        print("åŠ è½½Qwenç­–ç•¥æ¨¡å‹")
        print("=" * 80)
        self._load_policy()

        # åˆ›å»ºRL trainer
        print("\n" + "=" * 80)
        print("åˆ›å»ºRL Trainer")
        print("=" * 80)
        self._create_rl_trainer()

        # ç¯å¢ƒ
        self.train_envs = {}

        # ç»Ÿè®¡
        self.stats = {
            'epoch': 0,
            'best_score': 0.0,
            'best_scores': [],  # æ¯ä¸ªepochçš„æœ€ä½³åˆ†æ•°
            'avg_scores': [],
            'rewards_shaped': [],  # å¡‘å½¢åçš„å¥–åŠ±
            'checkpoints_saved': []
        }

        # æ—©åœ
        early_stop_config = config.get('early_stopping', {})
        self.early_stop_enabled = early_stop_config.get('enable', True)
        self.early_stop_patience = early_stop_config.get('patience', 5)
        self.early_stop_min_improvement = early_stop_config.get('min_improvement', 0.01)
        self.epochs_without_improvement = 0

        logger.info("âœ… ä¼˜åŒ–è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")

    def _load_policy(self):
        """åŠ è½½ç­–ç•¥"""
        policy_config = self.rl_config.get('policy', {})
        model_path = policy_config.get('model_path')

        if not model_path:
            raise ValueError("å¿…é¡»æŒ‡å®šmodel_path")

        self.policy = TrainableQwenPolicy(
            model_path=model_path,
            device=str(self.device),
            torch_dtype=torch.bfloat16,
            use_lora=policy_config.get('use_lora', True),
            lora_r=policy_config.get('lora_r', 32),
            lora_alpha=policy_config.get('lora_alpha', 64),
            value_head_hidden_dim=policy_config.get('value_head_dim', 2048)
        )

        self.policy.system_prompt = self.prompt_manager.get_system_prompt()

        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ: {model_path}")
        print(f"âœ“ LoRA: r={policy_config.get('lora_r', 32)}, alpha={policy_config.get('lora_alpha', 64)}")

    def _create_rl_trainer(self):
        """åˆ›å»ºRL trainer"""
        self.rl_trainer = RLTrainer(
            policy=self.policy,
            learning_rate=self.rl_config.get('learning_rate', 0.0003),
            value_coef=self.rl_config.get('value_coef', 1.0),
            entropy_coef=self.rl_config.get('entropy_coef', 0.1),
            max_grad_norm=self.rl_config.get('gradient_clip', 0.5),
            gamma=self.rl_config.get('gamma', 0.99),
            gae_lambda=self.rl_config.get('gae_lambda', 0.95),
            ppo_epochs=self.rl_config.get('ppo_epochs', 6),
            ppo_clip=self.rl_config.get('ppo_clip', 0.25),
            batch_size=self.rl_config.get('batch_size', 8),
            use_gigpo=self.rl_config.get('gigpo', {}).get('enable', True),
            gigpo_config={k: v for k, v in self.rl_config.get('gigpo', {}).items() if k != 'enable'},
            device=str(self.device)
        )

        print(f"âœ“ RL Traineråˆ›å»ºå®Œæˆ")

    def _create_environments(self):
        """åˆ›å»ºç¯å¢ƒ"""
        print("\n" + "=" * 80)
        print("åˆ›å»ºè®­ç»ƒç¯å¢ƒ")
        print("=" * 80)

        for dataset in self.train_datasets:
            logger.info(f"åˆ›å»º{dataset}ç¯å¢ƒ...")

            env = create_deep_workflow_env(
                dataset=dataset,
                opt_llm_config=self.env_config.get('opt_llm_config', {}),
                exec_llm_config=self.env_config.get('exec_llm_config', {}),
                operators=self.env_config.get('operators', []),
                env_num=self.env_config.get('env_num', 1),
                sample=self.env_config.get('sample', 12),
                max_rounds=self.env_config.get('max_rounds', 8),
                workspace_path=str(self.output_dir / 'workflows' / dataset)
            )

            self.train_envs[dataset] = env
            logger.info(f"âœ“ {dataset}ç¯å¢ƒåˆ›å»ºå®Œæˆ")

    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        logger.info(f"\n{'=' * 80}")
        logger.info(f"Epoch {epoch}/{self.total_epochs}")

        # è¯¾ç¨‹å­¦ä¹ ï¼šæ ¹æ®epochè°ƒæ•´æ ·æœ¬æ•°é‡
        sample_size = self.curriculum.get_sample_size(epoch)
        logger.info(f"ğŸ“š å½“å‰æ ·æœ¬æ•°: {sample_size} (è¯¾ç¨‹å­¦ä¹ )")
        logger.info(f"{'=' * 80}")

        epoch_stats = {
            'total_episodes': 0,
            'avg_score': 0.0,
            'avg_reward_shaped': 0.0,
            'num_updates': 0
        }

        # è®­ç»ƒ
        for dataset, env in self.train_envs.items():
            # åŠ¨æ€è°ƒæ•´ç¯å¢ƒçš„sampleå¤§å°
            env.sample = sample_size

            logger.info(f"\nè®­ç»ƒ {dataset}...")

            for update_iter in range(self.episodes_per_epoch // self.update_frequency):
                print(f"\n[{update_iter + 1}/{self.episodes_per_epoch // self.update_frequency}] æ”¶é›†æ•°æ®...")

                # æ”¶é›†rollout
                collection_stats = self.rl_trainer.collect_rollout(
                    env=env,
                    num_episodes=self.update_frequency,
                    max_steps_per_episode=self.env_config.get('max_rounds', 8)
                )

                # æ›´æ–°ç­–ç•¥
                print("æ›´æ–°ç­–ç•¥...")
                update_stats = self.rl_trainer.update()

                # ç»Ÿè®¡
                epoch_stats['total_episodes'] += collection_stats['num_episodes']
                epoch_stats['avg_score'] += collection_stats.get('avg_reward', 0.0)
                epoch_stats['num_updates'] += 1

                print(f"æ”¶é›†: {collection_stats}")
                print(f"æ›´æ–°: {update_stats}")

        # å¹³å‡ç»Ÿè®¡
        if epoch_stats['num_updates'] > 0:
            epoch_stats['avg_score'] /= epoch_stats['num_updates']
            epoch_stats['avg_reward_shaped'] /= epoch_stats['num_updates']

        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        self.stats['epoch'] = epoch
        self.stats['avg_scores'].append(epoch_stats['avg_score'])

        logger.info(f"\n{'=' * 80}")
        logger.info(f"Epoch {epoch} æ€»ç»“:")
        logger.info(f"  å¹³å‡åˆ†æ•°: {epoch_stats['avg_score']:.4f}")
        logger.info(f"  æ›´æ–°æ¬¡æ•°: {epoch_stats['num_updates']}")
        logger.info(f"{'=' * 80}")

        # æ‰“å°å…ƒå­¦ä¹ ç»Ÿè®¡
        logger.info(f"\n{'=' * 80}")
        logger.info("å…ƒå­¦ä¹ è¿›åº¦æŠ¥å‘Š - Meta Learning Progress")
        logger.info(f"{'=' * 80}")
        for dataset, env in self.train_envs.items():
            if hasattr(env, 'print_meta_statistics'):
                logger.info(f"\n{dataset}:")
                env.print_meta_statistics()

        return epoch_stats

    def save_checkpoint(self, epoch: int, score: float, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        # ä¿å­˜ç­–ç•¥
        checkpoint_name = f"best.pt" if is_best else f"epoch_{epoch}.pt"
        policy_path = self.checkpoint_dir / f"{checkpoint_name.replace('.pt', '_policy.pt')}"
        self.policy.save_checkpoint(str(policy_path))

        # ä¿å­˜trainer
        trainer_path = self.checkpoint_dir / f"{checkpoint_name.replace('.pt', '_trainer.pt')}"
        self.rl_trainer.save_checkpoint(str(trainer_path))

        # ä¿å­˜ç»Ÿè®¡
        stats_path = self.checkpoint_dir / f"{checkpoint_name.replace('.pt', '_stats.json')}"
        with open(stats_path, 'w') as f:
            json.dump(self.stats, f, indent=2)

        self.stats['checkpoints_saved'].append({
            'epoch': epoch,
            'score': score,
            'is_best': is_best,
            'path': str(policy_path)
        })

        logger.info(f"âœ“ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_name} (åˆ†æ•°: {score:.4f})")

        # ä¿å­˜å…ƒå­¦ä¹ æ£€æŸ¥ç‚¹
        for dataset, env in self.train_envs.items():
            if hasattr(env, 'save_meta_checkpoint'):
                env.save_meta_checkpoint()
                logger.info(f"âœ“ {dataset} å…ƒå­¦ä¹ æ£€æŸ¥ç‚¹å·²ä¿å­˜")

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        logger.info("\n" + "=" * 80)
        logger.info("å¼€å§‹ä¼˜åŒ–è®­ç»ƒ")
        logger.info("=" * 80)

        # åˆ›å»ºç¯å¢ƒ
        self._create_environments()

        # è®­ç»ƒå¾ªç¯
        for epoch in range(1, self.total_epochs + 1):
            # è®­ç»ƒ
            epoch_stats = self.train_epoch(epoch)

            # ä¿å­˜æ£€æŸ¥ç‚¹
            if epoch % self.save_frequency == 0:
                self.save_checkpoint(epoch, epoch_stats['avg_score'])

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³
            if epoch_stats['avg_score'] > self.stats['best_score']:
                improvement = epoch_stats['avg_score'] - self.stats['best_score']
                self.stats['best_score'] = epoch_stats['avg_score']
                self.save_checkpoint(epoch, epoch_stats['avg_score'], is_best=True)
                logger.info(f"ğŸ‰ æ–°æœ€ä½³åˆ†æ•°: {self.stats['best_score']:.4f} (+{improvement:.4f})")
                self.epochs_without_improvement = 0
            else:
                self.epochs_without_improvement += 1

            # æ—©åœæ£€æŸ¥
            if self.early_stop_enabled and self.epochs_without_improvement >= self.early_stop_patience:
                logger.info(f"\nâš ï¸ æ—©åœè§¦å‘: {self.early_stop_patience}ä¸ªepochæ— æ”¹è¿›")
                break

        logger.info("\n" + "=" * 80)
        logger.info("è®­ç»ƒå®Œæˆ!")
        logger.info(f"æ€»Epochæ•°: {self.stats['epoch']}")
        logger.info(f"æœ€ä½³åˆ†æ•°: {self.stats['best_score']:.4f}")
        logger.info(f"ä¿å­˜çš„æ£€æŸ¥ç‚¹æ•°: {len(self.stats['checkpoints_saved'])}")
        logger.info("=" * 80)


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(description="ä¼˜åŒ–çš„AIMEè®­ç»ƒ")
    parser.add_argument('--config', type=str, default='ä¼˜åŒ–è¿è¡Œ.yaml', help="é…ç½®æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    with open(args.config, 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)

    # æ›¿æ¢ç¯å¢ƒå˜é‡
    def replace_env_vars(obj):
        if isinstance(obj, dict):
            return {k: replace_env_vars(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [replace_env_vars(item) for item in obj]
        elif isinstance(obj, str) and obj.startswith('${') and obj.endswith('}'):
            env_var = obj[2:-1]
            return os.environ.get(env_var, obj)
        return obj

    config = replace_env_vars(config)

    # åˆ›å»ºtrainer
    trainer = OptimizedTrainer(config)

    # è®­ç»ƒ
    trainer.train()


if __name__ == "__main__":
    main()
