device: "cuda"
# 40分钟训练配置 - 快速观察Qwen学习和operator选择
total_epochs: 5  # 减少到5个epoch
episodes_per_epoch: 3  # 每个epoch 3个episode
update_frequency: 2  # 每2个episode更新一次
eval_episodes: 1  # 评估1个episode
save_frequency: 2

output_dir: "/content/drive/MyDrive/agentflow/outputs/quick_test"
experience_pool_size: 500

environment:
  train_datasets:
    - "AIME"
  test_datasets:
    - "AIME"
  data_path: "/content/agentflow/AFlow/data/AIME_2024.jsonl"

  # 优化LLM配置 - 提升推理能力
  opt_llm_config:
    model: "gpt-4o-mini"
    key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    temperature: 0.9

  exec_llm_config:
    model: "gpt-4o-mini"  # 保持使用mini观察性能
    key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    temperature: 0.7  # 降低temperature提高稳定性

  # 提供多样化的operators让Qwen学习选择
  operators:
    - "Custom"           # 通用operator - 适合数学推理
    # "CustomCodeGenerate" - 代码生成operator，不适合AIME数学题，已移除
    - "ScEnsemble"       # 自一致性集成
    - "Test"             # 测试operator
    - "Review"           # 审查operator
    - "Revise"           # 修订operator

  env_num: 2  # 2个并行环境,增加探索
  group_n: 1
  max_rounds: 5  # 每个episode最多5轮
  validation_rounds: 3
  sample: 24  # 训练集大小（总共30题，24训练+6测试）

  # 动态优化模式配置
  use_dynamic_optimizer: true  # 启用动态优化（MCTS + RL）
  rl_weight: 0.5  # RL权重，平衡MCTS UCB和RL Q-value

  # 新增：workflow内部采样数配置
  # 这控制ScEnsemble生成多少个候选方案
  workflow_sample_count: 15  # 快速测试使用15次采样（平衡速度和质量）

rl:
  policy:
    model_path: "/root/models/Qwen2.5-7B-Instruct"
    use_lora: true
    lora_r: 16
    lora_alpha: 32

  # GRPO训练参数 - 针对AIME小数据集优化（24个训练样本）
  learning_rate: 0.00005  # 中等学习率（介于minimal和full之间）
  batch_size: 4  # 小batch，适合少样本
  ppo_epochs: 3  # PPO epochs用于GRPO的多轮优化
  ppo_clip: 0.2
  gamma: 0.99  # 保留用于兼容性（GRPO不使用discount）
  entropy_coef: 0.1  # 提高探索性，防止operator使用退化
  gradient_clip: 1.0

logging:
  level: "INFO"

debug:
  verbose_env: true
