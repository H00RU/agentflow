"""
Deep Workflow Environment - MCTS + GRPOè®­ç»ƒç¯å¢ƒ
MCTS-based workflow optimization with GRPO training

Schema 1 æ¶æ„ï¼ˆèŒè´£åˆ†ç¦»ï¼‰:
- AFlow Optimizer: MCTSæ ‘æœç´¢æ‰¾æœ€ä¼˜workflow
- Qwen + GRPO: å­¦ä¼šç”Ÿæˆå¥½çš„workflowä¿®æ”¹å»ºè®®
- ä¸¤ä¸ªç³»ç»Ÿç‹¬ç«‹ï¼Œäº’ä¸å¹²æ‰°

å…³é”®æ”¹å˜ï¼šä½¿ç”¨åŸç”ŸAFlow Optimizerï¼Œè€Œä¸æ˜¯RLEnhancedOptimizer
"""

import sys
import os
import asyncio
import shutil
import importlib
from typing import List, Tuple, Dict, Optional
import numpy as np

# Add paths
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow', 'scripts'))

from scripts.logs import logger
from scripts.evaluator import DatasetType
from scripts.optimizer import Optimizer
from workflow_evaluator import WorkflowEvaluator

logger.info("[DeepWorkflowEnv] Using native AFlow Optimizer (Schema 1)")
logger.info("[DeepWorkflowEnv] Qwen + GRPO learns workflow optimization")


class DeepWorkflowEnv:
    """
    MCTS + GRPOè®­ç»ƒç¯å¢ƒï¼ˆSchema 1 - èŒè´£åˆ†ç¦»ï¼‰

    æ¶æ„ï¼ˆå®Œå…¨ç‹¬ç«‹çš„ä¸¤ä¸ªç³»ç»Ÿï¼‰:
    1. MCTSä¼˜åŒ–å™¨ï¼ˆAFlowï¼‰ï¼šé€šè¿‡æ ‘æœç´¢æ‰¾æœ€ä¼˜workflow
       - ä¸å—GRPOå½±å“
       - çº¯ç²¹çš„workflowä¼˜åŒ–
    2. Qwen + GRPOï¼šå­¦ä¼šç”Ÿæˆå¥½çš„workflowä¿®æ”¹å»ºè®®
       - è§‚å¯ŸMCTSè¿”å›çš„rewards
       - é€šè¿‡GRPOæ›´æ–°å‚æ•°

    æ•°æ®æµï¼š
    step() â†’ MCTSä¼˜åŒ– â†’ è·å¾—score â†’ Qwenä»scoreå­¦ä¹  â†’ GRPOæ›´æ–°

    è¿™ä¸ªæ¶æ„çš„ä¼˜åŠ¿ï¼š
    - æ¸…æ™°çš„èŒè´£åˆ†ç¦»ï¼ˆä¸¤ä¸ªç³»ç»Ÿäº’ä¸å¹²æ‰°ï¼‰
    - æ˜“äºè°ƒè¯•ï¼ˆMCTSé—®é¢˜å’Œå­¦ä¹ é—®é¢˜åˆ†ç¦»ï¼‰
    - ç†è®ºç®€æ´ï¼ˆæ ‡å‡†MCTS + æ ‡å‡†GRPOï¼‰
    """

    def __init__(
        self,
        dataset: str,
        opt_llm_config: Dict,
        exec_llm_config: Dict,
        operators: List[str],
        env_num: int = 2,
        sample: int = 3,
        max_rounds: int = 10,
        workspace_path: str = None,
        workflow_sample_count: int = None,
        validation_rounds: int = 3,
        rl_weight: float = 0.5,
        train_test_split: float = 0.8,
        use_qwen_code_generation: bool = False,
        qwen_code_generator=None,
        qwen_max_retries: int = 2,
        mini_batch_size: int = None
    ):
        """
        åˆå§‹åŒ–MCTS + GRPOè®­ç»ƒç¯å¢ƒ

        Args:
            dataset: æ•°æ®é›†åç§°ï¼ˆå¦‚"HumanEval", "AIME"ï¼‰
            opt_llm_config: ä¼˜åŒ–LLMé…ç½®ï¼ˆä¼ é€’ç»™MCTSä¼˜åŒ–å™¨ï¼Œä½†Qwenä¼šæ›¿ä»£ï¼‰
            exec_llm_config: æ‰§è¡ŒLLMé…ç½®ï¼ˆç”¨äºè¿è¡Œworkflowä¸­çš„LLMè°ƒç”¨ï¼‰
            operators: å¯ç”¨çš„operatorsåˆ—è¡¨
            env_num: å¹¶è¡Œç¯å¢ƒæ•°é‡
            sample: æ¯è½®æµ‹è¯•çš„æ ·æœ¬æ•°
            max_rounds: MCTSæœ€å¤§è½®æ•°
            workspace_path: workspaceè·¯å¾„ï¼ˆå­˜å‚¨workflowä»£ç ï¼‰
            workflow_sample_count: workflowå†…éƒ¨é‡‡æ ·æ•°ï¼ˆç”¨äºScEnsembleç­‰ï¼‰
            validation_rounds: MCTSéªŒè¯è½®æ•°
            rl_weight: MCTS UCBä¸RL Q-valueçš„èåˆæƒé‡ (0.0-1.0)
            train_test_split: è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†æ¯”ä¾‹ (é»˜è®¤0.8)
            use_qwen_code_generation: ä½¿ç”¨Qwenæ›¿ä»£GPT-4ç”Ÿæˆä»£ç 
            qwen_code_generator: Qwen policyå®ä¾‹ï¼ˆGRPOè®­ç»ƒçš„æ¨¡å‹ï¼‰
            qwen_max_retries: Qwenè¯­æ³•é”™è¯¯æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
            mini_batch_size: Mini-batchå¤§å°ï¼ˆNone=å…¨é‡ï¼‰
        """
        self.dataset = dataset
        self.opt_llm_config = opt_llm_config
        self.exec_llm_config = exec_llm_config
        self.operators = operators
        self.env_num = env_num
        self.sample = sample
        self.max_rounds = max_rounds
        self.workflow_sample_count = workflow_sample_count
        self.train_test_split = train_test_split
        self.validation_rounds = validation_rounds
        self.rl_weight = rl_weight

        # Mini-batch configuration
        self.mini_batch_size = mini_batch_size  # None = use all samples

        # MCTS + Qwenç›¸å…³å‚æ•°ï¼ˆä¿æŒå…¼å®¹æ€§ä½†ä¸ä½¿ç”¨ï¼‰
        # Schema 1ä¸­è¿™äº›å‚æ•°ä¸å†ä½¿ç”¨ï¼ŒMCTSå’ŒGRPOå®Œå…¨ç‹¬ç«‹
        self.use_qwen_code_generation = use_qwen_code_generation
        self.qwen_code_generator = qwen_code_generator
        self.qwen_max_retries = qwen_max_retries

        logger.info("[DeepWorkflowEnv] Schema 1 Configuration:")
        logger.info(f"  - Qwen code generation: {use_qwen_code_generation} (ignored in Schema 1)")
        logger.info(f"  - RL weight: {rl_weight} (ignored in Schema 1)")

        # Workspaceè·¯å¾„ï¼ˆå­˜å‚¨ç”Ÿæˆçš„workflowï¼‰
        # âš ï¸ å¿…é¡»ä½¿ç”¨AFlowç›®å½•ä¸‹çš„è·¯å¾„ï¼Œç¡®ä¿Pythonæ¨¡å—å¯¼å…¥æ­£ç¡®
        import sys
        aflow_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
        aflow_optimized_path = os.path.join(aflow_path, 'optimized')

        # æ·»åŠ AFlow/optimizedåˆ°sys.pathï¼ˆè¿™æ ·Optimizerå¯ä»¥æ­£ç¡®å¯¼å…¥workflowæ¨¡å—ï¼‰
        aflow_optimized_path = os.path.abspath(aflow_optimized_path)
        if aflow_optimized_path not in sys.path:
            sys.path.insert(0, aflow_optimized_path)

        if workspace_path is None:
            # ä½¿ç”¨ç›¸å¯¹äºAFlow/optimizedçš„è·¯å¾„
            self.workspace_path = os.path.join(aflow_optimized_path, dataset)
        else:
            # å¦‚æœæŒ‡å®šäº†custom workspaceï¼Œä½¿ç”¨AFlow/optimized
            logger.info(f"[DeepWorkflowEnv] Custom workspace specified: {workspace_path}")
            logger.info(f"[DeepWorkflowEnv] Using AFlow default workspace for proper import")
            self.workspace_path = os.path.join(aflow_optimized_path, dataset)

        os.makedirs(self.workspace_path, exist_ok=True)
        logger.info(f"[DeepWorkflowEnv] Workspace: {self.workspace_path}")

        # åˆ›å»ºevaluatorï¼ˆç”¨äºçœŸå®æµ‹è¯•ï¼‰
        # âš ï¸ å¿…é¡»åœ¨_init_dynamic_mode()ä¹‹å‰åˆ›å»ºï¼Œå› ä¸ºé€‚é…å™¨éœ€è¦å®ƒ
        # æ‰€æœ‰æ•°æ®é›†ç»Ÿä¸€ä½¿ç”¨WorkflowEvaluatorï¼ŒAIMEå·²åŠ å…¥AFlowæ ‡å‡†æ”¯æŒ
        self.evaluator = WorkflowEvaluator(
            dataset=self.dataset,
            sample_size=sample,
            timeout_per_problem=30,
            train_test_split=self.train_test_split,
            llm_config=self.exec_llm_config  # ä¼ é€’LLMé…ç½®ç»™evaluator
        )
        logger.info(f"[DeepWorkflowEnv] Using WorkflowEvaluator for {self.dataset}")

        # åˆå§‹åŒ–MCTSä¼˜åŒ–å™¨å’Œç›¸å…³ç»„ä»¶
        logger.info(f"[DeepWorkflowEnv] âœ¨ Initializing MCTS + GRPO environment")
        logger.info(f"[DeepWorkflowEnv] Qwen will replace GPT-4 in MCTS framework")
        self._init_mcts_components()

        # å½“å‰çŠ¶æ€
        self.current_round = 0
        self.workflow_history = []  # å†å²workflowåŠå…¶åˆ†æ•°
        self.best_score = 0.0
        self.best_workflow = None

        # ç»Ÿè®¡
        self.total_api_calls = 0
        self.total_tests_run = 0

        logger.info(f"[DeepWorkflowEnv] Initialized")
        logger.info(f"[DeepWorkflowEnv] Dataset: {dataset}")
        logger.info(f"[DeepWorkflowEnv] Workspace: {self.workspace_path}")
        logger.info(f"[DeepWorkflowEnv] Evaluator sample size: {sample}")
        if self.mini_batch_size:
            logger.info(f"[DeepWorkflowEnv] ğŸ² Mini-Batch Mode: {self.mini_batch_size} problems/test (random sampling)")
        else:
            logger.info(f"[DeepWorkflowEnv] ğŸ“Š Full-Batch Mode: {sample} problems/test")
        logger.info(f"[DeepWorkflowEnv] âœ… REAL WORKFLOW EXECUTION ENABLED")

    def _init_mcts_components(self):
        """åˆå§‹åŒ–åŸç”ŸMCTSä¼˜åŒ–å™¨ï¼ˆSchema 1ï¼‰"""
        # Schema 1: ä¸ä½¿ç”¨å…±äº«ç»éªŒæ± å’ŒçŠ¶æ€ç®¡ç†ï¼ŒMCTSå’ŒGRPOå®Œå…¨ç‹¬ç«‹

        # ä¸ºæ¯ä¸ªå¹¶è¡Œç¯å¢ƒåˆ›å»ºä¸€ä¸ªåŸç”ŸOptimizerå®ä¾‹
        self.optimizers = []
        question_type = self._infer_question_type(self.dataset)

        for i in range(self.env_num):
            # ä¼ ç»™Optimizerçš„optimized_pathåº”è¯¥æ˜¯ç›¸å¯¹äºAFlow/optimizedçš„ç›¸å¯¹è·¯å¾„
            # è¿™æ ·Optimizerå¯ä»¥æ­£ç¡®æ„é€ å¯¼å…¥è·¯å¾„
            optimizer = Optimizer(
                dataset=self.dataset,
                question_type=question_type,
                opt_llm_config=self.opt_llm_config,
                exec_llm_config=self.exec_llm_config,
                operators=self.operators,
                sample=self.sample,
                check_convergence=False,
                optimized_path="optimized/",  # ç›¸å¯¹è·¯å¾„ï¼Œå·²åŒ…å«æ•°æ®é›†å­ç›®å½•
                initial_round=1,
                max_rounds=self.max_rounds,
            )
            self.optimizers.append(optimizer)

        # ===== Schema 1: ä½¿ç”¨WorkflowEvaluatorè¿›è¡Œè¯„ä¼° =====
        # æ›¿æ¢optimizerçš„evaluation_utilsä¸ºWorkflowEvaluator
        from evaluation_adapter import EvaluationUtilsAdapter

        for optimizer in self.optimizers:
            optimizer.evaluation_utils = EvaluationUtilsAdapter(
                workflow_evaluator=self.evaluator,
                root_path=optimizer.root_path
            )

        logger.info(f"[DeepWorkflowEnv] Created {self.env_num} native Optimizers (Schema 1)")
        logger.info(f"[DeepWorkflowEnv] âœ… MCTS and GRPO are completely independent")
        logger.info(f"[DeepWorkflowEnv] âœ… Using WorkflowEvaluator for {self.dataset}")
        logger.info(f"[DeepWorkflowEnv] âœ… Mini-batch and train/test split enabled")

    def _infer_question_type(self, dataset: str) -> str:
        """æ¨æ–­é—®é¢˜ç±»å‹"""
        dataset_upper = dataset.upper()
        if dataset_upper in ["HUMANEVAL", "MBPP", "CODEEVAL"]:
            return "code"
        elif dataset_upper in ["AIME", "MATH", "GSM8K"]:
            return "math"
        else:
            return "qa"

    def reset(self) -> Tuple[List[str], List[Dict]]:
        """
        é‡ç½®ç¯å¢ƒ

        Returns:
            observations: è§‚æµ‹åˆ—è¡¨
            info: ä¿¡æ¯å­—å…¸åˆ—è¡¨
        """
        self.current_round = 0

        observations = []
        info = []

        for i in range(self.env_num):
            # æ„é€ è§‚æµ‹ï¼šå‘Šè¯‰Qwenå½“å‰çŠ¶æ€
            obs = self._construct_observation(
                round_num=0,
                best_score=self.best_score,
                history_summary=self._get_history_summary()
            )
            observations.append(obs)

            info_dict = {
                'step': 0,
                'round': 0,
                'env_id': i,
                'best_score': self.best_score,
                'workflow_path': None
            }
            info.append(info_dict)

        logger.info(f"[DeepWorkflowEnv] Environment reset")
        return observations, info

    def step(self, actions: List[str]) -> Tuple[List[str], List[float], List[bool], List[Dict]]:
        """
        æ‰§è¡ŒMCTSä¼˜åŒ–stepï¼ˆSchema 1ï¼‰

        æµç¨‹ï¼ˆä¸¤ä¸ªç‹¬ç«‹ç³»ç»Ÿï¼‰:
        1. ç¯å¢ƒï¼ˆstepï¼‰æ¥æ”¶Qwenå»ºè®®ä½œä¸ºè¾“å…¥ï¼ˆä½†ä¸ç›´æ¥ä½¿ç”¨ï¼‰
        2. MCTSä¼˜åŒ–å™¨æ‰§è¡Œçº¯ç²¹çš„æ ‘æœç´¢ï¼Œæ‰¾æœ€ä¼˜workflow
        3. è¯„ä¼°workflowæ€§èƒ½
        4. è¿”å›çœŸå®pass@kåˆ†æ•°ä½œä¸ºreward
        5. Qwen + GRPOè§‚å¯Ÿè¿™ä¸ªrewardï¼Œä»ä¸­å­¦ä¹ 

        Args:
            actions: Qwenç”Ÿæˆçš„workflowä¼˜åŒ–å»ºè®®åˆ—è¡¨ï¼ˆå¯é€‰ä½¿ç”¨ï¼‰

        Returns:
            next_observations: ä¸‹ä¸€æ­¥è§‚æµ‹
            rewards: çœŸå®çš„workflowæ€§èƒ½åˆ†æ•°ï¼ˆpass@kï¼‰
            dones: æ˜¯å¦ç»“æŸ
            info: é¢å¤–ä¿¡æ¯
        """
        self.current_round += 1
        return self._step_mcts(actions)

    def _step_mcts(self, actions: List[str]) -> Tuple[List[str], List[float], List[bool], List[Dict]]:
        """
        MCTSä¼˜åŒ–stepå®ç°ï¼ˆSchema 1 - çº¯MCTSæœç´¢ï¼‰

        ä½¿ç”¨åŸç”ŸOptimizerè¿›è¡Œçº¯ç²¹çš„MCTSæ ‘æœç´¢ï¼š
        1. Optimizeræ‰§è¡ŒMCTSæ ‘æœç´¢ï¼ˆä¸GRPOå®Œå…¨ç‹¬ç«‹ï¼‰
        2. ç”Ÿæˆæœ€ä¼˜workflow
        3. åœ¨çœŸå®éªŒè¯é›†ä¸Šè¯„ä¼°
        4. è¿”å›pass@kåˆ†æ•°
        """
        next_observations = []
        rewards = []
        dones = []
        info = []

        logger.info(f"[DeepWorkflowEnv] ===== Round {self.current_round} (MCTS + GRPO) =====")
        logger.info(f"[DeepWorkflowEnv] Running {len(actions)} MCTS optimizations...")

        # å¹¶è¡Œè¿è¡Œæ‰€æœ‰ä¼˜åŒ–å™¨
        for i, (optimizer, action) in enumerate(zip(self.optimizers, actions)):
            try:
                logger.info(f"[DeepWorkflowEnv] Env {i}: Running native MCTS Optimizer...")
                logger.info(f"[DeepWorkflowEnv] Env {i}: Action hint: {action[:100]}...")

                # è¿è¡Œä¸€è½®ä¼˜åŒ–ï¼ˆçº¯MCTSï¼Œä¸GRPOå®Œå…¨ç‹¬ç«‹ï¼‰
                # Optimizer ä¼šï¼š
                # 1. æ‰§è¡ŒMCTSæ ‘æœç´¢ï¼ˆUCBç­–ç•¥ï¼‰
                # 2. ä½¿ç”¨LLMç”Ÿæˆæ–°workflow
                # 3. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                # 4. æ›´æ–°MCTSæ ‘ï¼ˆä¸æ¶‰åŠRLï¼‰
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                try:
                    score = loop.run_until_complete(optimizer._optimize_graph())
                except Exception as e:
                    logger.error(f"[DeepWorkflowEnv] Env {i}: Error in optimization: {e}")
                    import traceback
                    traceback.print_exc()
                    score = 0.0
                finally:
                    loop.close()

                logger.info(f"[DeepWorkflowEnv] Env {i}: âœ… Optimization score: {score:.4f}")

                # æ›´æ–°æœ€ä½³åˆ†æ•°
                if score > self.best_score:
                    logger.info(f"[DeepWorkflowEnv] Env {i}: ğŸ‰ NEW BEST SCORE! {self.best_score:.4f} -> {score:.4f}")
                    self.best_score = score
                    self.best_workflow = optimizer.graph

                # è®°å½•å†å²
                self.workflow_history.append({
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'workflow_type': 'dynamic'
                })

                # è¿”å›åˆ†æ•°ä½œä¸º reward
                reward = float(score)
                rewards.append(reward)

                # æ„é€ ä¸‹ä¸€ä¸ªè§‚æµ‹
                next_obs = self._construct_observation(
                    round_num=self.current_round,
                    best_score=self.best_score,
                    history_summary=self._get_history_summary(),
                    last_score=score
                )
                next_observations.append(next_obs)

                # åˆ¤æ–­æ˜¯å¦ç»“æŸ
                done = self.current_round >= self.max_rounds
                dones.append(done)

                # Info
                info_dict = {
                    'step': self.current_round,
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'best_score': self.best_score,
                    'is_best': score == self.best_score,
                    'optimization_type': 'dynamic_rl_mcts'
                }
                info.append(info_dict)

            except Exception as e:
                logger.error(f"[DeepWorkflowEnv] Env {i}: ERROR: {e}")
                import traceback
                traceback.print_exc()

                rewards.append(0.0)
                next_observations.append(self._construct_observation(
                    self.current_round, self.best_score, f"Error: {str(e)}"
                ))
                dones.append(False)
                info.append({'step': self.current_round, 'error': str(e)})

        avg_reward = np.mean(rewards) if rewards else 0.0
        logger.info(f"[DeepWorkflowEnv] Round {self.current_round} completed")
        logger.info(f"[DeepWorkflowEnv] Avg reward: {avg_reward:.4f}, Best so far: {self.best_score:.4f}")

        return next_observations, rewards, dones, info

    def _extract_code_from_qwen(self, qwen_output: str) -> Optional[Dict[str, str]]:
        """
        ä»Qwenè¾“å‡ºæå–ä»£ç  - å®Œå…¨å¯¹é½åŸç‰ˆAFlow

        åŸç‰ˆAFlowæœŸæœ›LLMè¿”å›ï¼š
        <modification>...</modification>
        <graph>...</graph>
        <prompt>...</prompt>

        Args:
            qwen_output: Qwenç”Ÿæˆçš„è¾“å‡º

        Returns:
            {'graph': str, 'modification': str, 'prompt': str} æˆ– None
        """
        import re

        result = {}

        # æå– modification
        modification_pattern = r"<modification>(.*?)</modification>"
        modification_match = re.search(modification_pattern, qwen_output, re.DOTALL)
        if modification_match:
            result['modification'] = modification_match.group(1).strip()
        else:
            result['modification'] = "No modification description provided"

        # æå– graph (å¿…éœ€)
        graph_pattern = r"<graph>(.*?)</graph>"
        graph_match = re.search(graph_pattern, qwen_output, re.DOTALL)
        if not graph_match:
            logger.error("[DeepWorkflowEnv] No <graph> tag found in Qwen output")
            return None

        result['graph'] = graph_match.group(1).strip()

        # æå– prompt (å¯é€‰)
        prompt_pattern = r"<prompt>(.*?)</prompt>"
        prompt_match = re.search(prompt_pattern, qwen_output, re.DOTALL)
        if prompt_match:
            result['prompt'] = prompt_match.group(1).strip()
        else:
            result['prompt'] = "# Auto-generated - no custom prompts needed\n"

        return result

    def _validate_python_syntax(self, code: str) -> bool:
        """
        éªŒè¯Pythonä»£ç è¯­æ³•

        Args:
            code: Pythonä»£ç å­—ç¬¦ä¸²

        Returns:
            bool: è¯­æ³•æ˜¯å¦æ­£ç¡®
        """
        try:
            compile(code, '<string>', 'exec')
            return True
        except SyntaxError as e:
            logger.error(f"[DeepWorkflowEnv] Syntax error in code: {e}")
            logger.error(f"[DeepWorkflowEnv] Error line: {e.lineno}, offset: {e.offset}")
            logger.error(f"[DeepWorkflowEnv] Error text: {e.text}")
            return False
        except Exception as e:
            logger.error(f"[DeepWorkflowEnv] Unexpected error during syntax check: {e}")
            return False

    def _save_workflow_code_aflow_style(self,
                                        graph_code: str,
                                        prompt_code: str,
                                        round_id: str,
                                        modification: str) -> str:
        """
        ä½¿ç”¨åŸç‰ˆAFlowçš„æ–¹å¼ä¿å­˜workflowä»£ç 

        åŸç‰ˆAFlow: graph_utils.py:147-158
        - ä½¿ç”¨WORKFLOW_TEMPLATEå¡«å……
        - ä¿å­˜graph.py, prompt.py, __init__.py
        - ä¿å­˜modification.txtè®°å½•

        Args:
            graph_code: workflow classä»£ç 
            prompt_code: promptå®šä¹‰ä»£ç 
            round_id: roundæ ‡è¯†
            modification: ä¿®æ”¹æè¿°

        Returns:
            str: graph.pyçš„å®Œæ•´è·¯å¾„
        """
        from scripts.prompts.optimize_prompt import WORKFLOW_TEMPLATE

        # åˆ›å»ºroundç›®å½• - ç»Ÿä¸€ä½¿ç”¨workflowså­ç›®å½•ä»¥åŒ¹é…optimizeræŸ¥æ‰¾è·¯å¾„
        workflows_base = os.path.join(self.workspace_path, "workflows")
        round_dir = os.path.join(workflows_base, f"round_{round_id}")
        os.makedirs(round_dir, exist_ok=True)

        # 1. ä½¿ç”¨WORKFLOW_TEMPLATEç”Ÿæˆå®Œæ•´ä»£ç ï¼ˆä¸åŸç‰ˆAFlowç›¸åŒï¼‰
        full_graph_code = WORKFLOW_TEMPLATE.format(
            graph=graph_code,
            round=round_id,
            dataset=self.dataset
        )

        # 2. ä¿å­˜graph.py
        graph_path = os.path.join(round_dir, "graph.py")
        with open(graph_path, 'w', encoding='utf-8') as f:
            f.write(full_graph_code)

        # 3. ä¿å­˜prompt.py
        prompt_path = os.path.join(round_dir, "prompt.py")
        with open(prompt_path, 'w', encoding='utf-8') as f:
            f.write(prompt_code)

        # 4. ä¿å­˜__init__.py
        init_path = os.path.join(round_dir, "__init__.py")
        with open(init_path, 'w', encoding='utf-8') as f:
            f.write("")

        # 5. ä¿å­˜modification.txtè®°å½•
        modification_path = os.path.join(round_dir, "modification.txt")
        with open(modification_path, 'w', encoding='utf-8') as f:
            f.write(f"Round {round_id} Modification:\n")
            f.write(f"{modification}\n\n")
            f.write("This workflow was generated by Qwen (no Parser).\n")
            f.write("Fully aligned with original AFlow design.\n")

        logger.info(f"[DeepWorkflowEnv] Saved workflow files to {round_dir}")
        logger.info(f"[DeepWorkflowEnv]   - graph.py: {len(full_graph_code)} chars")
        logger.info(f"[DeepWorkflowEnv]   - prompt.py: {len(prompt_code)} chars")
        logger.info(f"[DeepWorkflowEnv]   - modification.txt")

        return graph_path

    def _construct_observation(
        self,
        round_num: int,
        best_score: float,
        history_summary: str,
        last_score: Optional[float] = None
    ) -> str:
        """æ„é€ ç»™Qwençš„è§‚æµ‹ï¼Œæ ¹æ®datasetç”Ÿæˆç›¸åº”çš„ä»»åŠ¡æè¿°"""
        # æ ¹æ®datasetç”Ÿæˆä»»åŠ¡æè¿°
        dataset_upper = self.dataset.upper()
        if dataset_upper == "AIME":
            task_desc = "Design and optimize agent workflow for solving AIME mathematical problems"
            focus_points = """1. Which operators to use for mathematical reasoning
2. How to combine them effectively for problem-solving
3. Using ensemble methods for robust solutions
4. How to improve upon previous attempts"""
        elif dataset_upper == "HUMANEVAL":
            task_desc = "Design and optimize agent workflow for code generation"
            focus_points = """1. Which operators to use for code generation
2. How to combine them effectively
3. How to improve upon previous attempts"""
        else:
            # é€šç”¨æè¿°
            task_desc = f"Design and optimize agent workflow for {self.dataset} tasks"
            focus_points = """1. Which operators to use
2. How to combine them effectively
3. How to improve upon previous attempts"""

        obs = f"""Dataset: {self.dataset}
Task: {task_desc}
Round: {round_num}/{self.max_rounds}

Current Best Score: {best_score:.4f}"""

        if last_score is not None:
            obs += f"\nLast Score: {last_score:.4f}"

        obs += f"""

Available Operators:
{', '.join(self.operators)}

{history_summary}

Your task: Generate a workflow description that will be converted to executable code and tested on real {self.dataset} problems. Focus on:
{focus_points}
"""

        return obs

    def _get_history_summary(self) -> str:
        """è·å–å†å²workflowæ‘˜è¦"""
        if not self.workflow_history:
            return "History: No previous workflows yet."

        # å–æœ€è¿‘3ä¸ªworkflow
        recent = self.workflow_history[-3:]
        summary = "Recent Workflow Performance:\n"

        for item in recent:
            summary += f"  Round {item['round']} Env{item['env_id']}: "
            summary += f"Score={item['score']:.4f}\n"

        return summary

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        logger.info(f"[DeepWorkflowEnv] Environment closed")
        logger.info(f"[DeepWorkflowEnv] Total tests run: {self.total_tests_run}")
        logger.info(f"[DeepWorkflowEnv] Best score achieved: {self.best_score:.4f}")


def create_deep_workflow_env(dataset, opt_llm_config, exec_llm_config, **kwargs):
    """
    åˆ›å»ºMCTS + GRPOè®­ç»ƒç¯å¢ƒçš„å·¥å‚å‡½æ•°ï¼ˆSchema 1ï¼‰

    æ¶æ„ï¼ˆèŒè´£åˆ†ç¦»ï¼‰ï¼š
    - MCTSä¼˜åŒ–å™¨ï¼ˆAFlowï¼‰ï¼šæ‰¾æœ€ä¼˜workflow
    - Qwen + GRPOï¼šå­¦ä¼šç”Ÿæˆå¥½çš„ä¿®æ”¹å»ºè®®

    ä¸¤ä¸ªç³»ç»Ÿå®Œå…¨ç‹¬ç«‹ï¼Œäº’ä¸å¹²æ‰°ã€‚

    å…³é”®å‚æ•°ï¼š
    - max_rounds: MCTSæ ‘æœç´¢æœ€å¤§è½®æ•°
    - sample: æ¯è½®è¯„ä¼°çš„æ ·æœ¬æ•°
    - train_test_split: è®­ç»ƒ/æµ‹è¯•é›†åˆ’åˆ†æ¯”ä¾‹
    - mini_batch_size: å°æ‰¹é‡æµ‹è¯•å¤§å°ï¼ˆNone=å…¨é‡ï¼‰
    """
    return DeepWorkflowEnv(
        dataset=dataset,
        opt_llm_config=opt_llm_config,
        exec_llm_config=exec_llm_config,
        operators=kwargs.get('operators', ['Custom', 'CustomCodeGenerate', 'ScEnsemble', 'Test']),
        env_num=kwargs.get('env_num', 2),
        sample=kwargs.get('sample', 3),
        max_rounds=kwargs.get('max_rounds', 10),
        workspace_path=kwargs.get('workspace_path'),
        workflow_sample_count=kwargs.get('workflow_sample_count'),
        validation_rounds=kwargs.get('validation_rounds', 3),
        rl_weight=kwargs.get('rl_weight', 0.5),
        train_test_split=kwargs.get('train_test_split', 0.8),
        # MCTS + Qwenå‚æ•°
        use_qwen_code_generation=kwargs.get('use_qwen_code_generation', False),
        qwen_code_generator=kwargs.get('qwen_code_generator'),
        qwen_max_retries=kwargs.get('qwen_max_retries', 2),
        # Mini-batchå‚æ•°
        mini_batch_size=kwargs.get('mini_batch_size', None)
    )


if __name__ == "__main__":
    # æµ‹è¯•ç¯å¢ƒ
    import yaml

    # åŠ è½½é…ç½®
    config_path = "deep_config_e2e.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    env_config = config['environment']

    # åˆ›å»ºç¯å¢ƒ
    env = create_deep_workflow_env(
        dataset="HumanEval",
        opt_llm_config=env_config['opt_llm_config'],
        exec_llm_config=env_config['exec_llm_config'],
        operators=env_config['operators'],
        env_num=1,
        sample=2
    )

    # æµ‹è¯•
    obs, info = env.reset()
    print(f"Initial observation:\n{obs[0]}\n")

    # æ¨¡æ‹ŸQwenè¾“å‡º
    test_action = """
<workflow_modification>
Use ensemble approach to improve code quality
</workflow_modification>

<operators>
CustomCodeGenerate, ScEnsemble
</operators>

<workflow_steps>
1. Generate 3 candidate code solutions
2. Use ScEnsemble to select the best one
</workflow_steps>
"""

    next_obs, rewards, dones, info = env.step([test_action])
    print(f"Reward (real pass@k): {rewards[0]:.4f}")
    print(f"This is a REAL score from executing workflow on HumanEval!")
