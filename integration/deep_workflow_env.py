"""
Deep Workflow Environment - çœŸæ­£çš„AFlow Workflowæ‰§è¡Œç¯å¢ƒ
Real AFlow workflow execution environment with actual code testing

æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
1. Static Mode (é»˜è®¤): ä½¿ç”¨ WorkflowParser ç”Ÿæˆå›ºå®šä»£ç 
2. Dynamic Mode: ä½¿ç”¨ RLEnhancedOptimizer åŠ¨æ€ä¼˜åŒ–
"""

import sys
import os
import asyncio
import shutil
import importlib
from typing import List, Tuple, Dict, Optional
import numpy as np

# Add paths
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow', 'scripts'))

from scripts.logs import logger
from scripts.evaluator import DatasetType
from workflow_parser import WorkflowParser, WorkflowSpec, DatasetClassifier
from workflow_evaluator import WorkflowEvaluator

# å°è¯•å¯¼å…¥åŠ¨æ€ä¼˜åŒ–ç»„ä»¶
try:
    from scripts.optimizer_rl import RLEnhancedOptimizer
    from scripts.shared_experience import SharedExperiencePool, Experience
    from unified_state import WorkflowState, StateManager
    DYNAMIC_OPTIMIZER_AVAILABLE = True
except ImportError as e:
    logger.warning(f"Dynamic optimizer not available: {e}")
    DYNAMIC_OPTIMIZER_AVAILABLE = False
    RLEnhancedOptimizer = None
    SharedExperiencePool = None
    StateManager = None


class DeepWorkflowEnv:
    """
    æ·±åº¦é›†æˆçš„Workflowç¯å¢ƒ

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    1. Static Mode (é»˜è®¤): æ¥æ”¶Qwenæè¿° â†’ WorkflowParser â†’ å›ºå®šä»£ç 
    2. Dynamic Mode: ä½¿ç”¨ RLEnhancedOptimizer â†’ MCTS + RL â†’ åŠ¨æ€ä¼˜åŒ–

    åŠŸèƒ½ï¼š
    1. æ¥æ”¶Qwenç”Ÿæˆçš„workflowæè¿°ï¼ˆStaticï¼‰æˆ–ä¼˜åŒ–å»ºè®®ï¼ˆDynamicï¼‰
    2. ç”Ÿæˆå¹¶æ‰§è¡Œworkflowï¼ˆé™æ€oråŠ¨æ€ï¼‰
    3. è¿”å›çœŸå®çš„pass@kåˆ†æ•°ä½œä¸ºreward
    """

    def __init__(
        self,
        dataset: str,
        opt_llm_config: Dict,
        exec_llm_config: Dict,
        operators: List[str],
        env_num: int = 2,
        sample: int = 3,
        max_rounds: int = 10,
        workspace_path: str = None,
        workflow_sample_count: int = None,
        use_dynamic_optimizer: bool = False,
        validation_rounds: int = 3,
        rl_weight: float = 0.5
    ):
        """
        åˆå§‹åŒ–çœŸå®workflowç¯å¢ƒ

        Args:
            dataset: æ•°æ®é›†åç§°ï¼ˆå¦‚"HumanEval", "AIME"ï¼‰
            opt_llm_config: ä¼˜åŒ–LLMé…ç½®ï¼ˆGPT-4oï¼Œç”¨äºworkflowç”Ÿæˆï¼‰
            exec_llm_config: æ‰§è¡ŒLLMé…ç½®ï¼ˆç”¨äºè¿è¡Œworkflowä¸­çš„LLMè°ƒç”¨ï¼‰
            operators: å¯ç”¨çš„operatorsåˆ—è¡¨
            env_num: å¹¶è¡Œç¯å¢ƒæ•°é‡
            sample: æ¯è½®æµ‹è¯•çš„æ ·æœ¬æ•°
            max_rounds: æœ€å¤§è½®æ•°
            workspace_path: workspaceè·¯å¾„ï¼ˆå­˜å‚¨workflowä»£ç ï¼‰
            workflow_sample_count: workflowå†…éƒ¨é‡‡æ ·æ•°ï¼ˆç”¨äºScEnsembleç­‰ï¼‰
            use_dynamic_optimizer: æ˜¯å¦ä½¿ç”¨åŠ¨æ€ä¼˜åŒ–å™¨ (é»˜è®¤Falseä¿æŒå‘åå…¼å®¹)
            validation_rounds: éªŒè¯è½®æ•° (ä»…åŠ¨æ€æ¨¡å¼)
            rl_weight: RLæƒé‡ (ä»…åŠ¨æ€æ¨¡å¼ï¼Œ0.0-1.0)
        """
        self.dataset = dataset
        self.opt_llm_config = opt_llm_config
        self.exec_llm_config = exec_llm_config
        self.operators = operators
        self.env_num = env_num
        self.sample = sample
        self.max_rounds = max_rounds
        self.workflow_sample_count = workflow_sample_count
        self.use_dynamic_optimizer = use_dynamic_optimizer
        self.validation_rounds = validation_rounds
        self.rl_weight = rl_weight

        # æ£€æŸ¥åŠ¨æ€æ¨¡å¼æ˜¯å¦å¯ç”¨
        if use_dynamic_optimizer and not DYNAMIC_OPTIMIZER_AVAILABLE:
            logger.error("[DeepWorkflowEnv] Dynamic optimizer requested but not available!")
            logger.error("[DeepWorkflowEnv] Falling back to static mode.")
            self.use_dynamic_optimizer = False

        # Workspaceè·¯å¾„ï¼ˆå­˜å‚¨ç”Ÿæˆçš„workflowï¼‰
        if workspace_path is None:
            aflow_path = os.path.join(os.path.dirname(__file__), '..', 'AFlow')
            if self.use_dynamic_optimizer:
                self.workspace_path = os.path.join(aflow_path, 'optimized', dataset)
            else:
                self.workspace_path = os.path.join(aflow_path, 'workspace', dataset, 'workflows_rl')
        else:
            self.workspace_path = workspace_path

        os.makedirs(self.workspace_path, exist_ok=True)

        # æ ¹æ®æ¨¡å¼åˆå§‹åŒ–ç»„ä»¶
        if self.use_dynamic_optimizer:
            # åŠ¨æ€æ¨¡å¼ï¼šåˆ›å»ºå…±äº«ç»éªŒæ± å’Œä¼˜åŒ–å™¨
            logger.info(f"[DeepWorkflowEnv] âœ¨ DYNAMIC MODE: Using RLEnhancedOptimizer")
            self._init_dynamic_mode()
        else:
            # é™æ€æ¨¡å¼ï¼šåˆ›å»ºworkflowè§£æå™¨
            logger.info(f"[DeepWorkflowEnv] ğŸ“‹ STATIC MODE: Using WorkflowParser")
            self.workflow_parser = WorkflowParser()

        # åˆ›å»ºevaluatorï¼ˆç”¨äºçœŸå®æµ‹è¯•ï¼Œæ ¹æ®datasetç±»å‹åŠ¨æ€é€‰æ‹©ï¼‰
        # ä¼˜å…ˆå°è¯•ä½¿ç”¨ç‰¹å®šæ•°æ®é›†çš„evaluatorï¼Œå¦åˆ™ä½¿ç”¨é€šç”¨evaluator
        if self._has_custom_evaluator(self.dataset):
            self.evaluator = self._create_custom_evaluator(self.dataset, self.exec_llm_config)
            logger.info(f"[DeepWorkflowEnv] Using custom evaluator for {self.dataset}")
        else:
            self.evaluator = WorkflowEvaluator(
                dataset=self.dataset,
                sample_size=sample,
                timeout_per_problem=30
            )
            logger.info(f"[DeepWorkflowEnv] Using WorkflowEvaluator for {self.dataset}")

        # å½“å‰çŠ¶æ€
        self.current_round = 0
        self.workflow_history = []  # å†å²workflowåŠå…¶åˆ†æ•°
        self.best_score = 0.0
        self.best_workflow = None

        # ç»Ÿè®¡
        self.total_api_calls = 0
        self.total_tests_run = 0

        logger.info(f"[DeepWorkflowEnv] Initialized")
        logger.info(f"[DeepWorkflowEnv] Dataset: {dataset}")
        logger.info(f"[DeepWorkflowEnv] Workspace: {self.workspace_path}")
        logger.info(f"[DeepWorkflowEnv] Evaluator sample size: {sample}")
        logger.info(f"[DeepWorkflowEnv] âœ… REAL WORKFLOW EXECUTION ENABLED")

    def _init_dynamic_mode(self):
        """åˆå§‹åŒ–åŠ¨æ€ä¼˜åŒ–æ¨¡å¼çš„ç»„ä»¶"""
        # åˆ›å»ºå…±äº«ç»éªŒæ± å’ŒçŠ¶æ€ç®¡ç†å™¨
        self.shared_experience_pool = SharedExperiencePool(max_size=10000)
        self.state_manager = StateManager()

        # ä¸ºæ¯ä¸ªå¹¶è¡Œç¯å¢ƒåˆ›å»ºä¸€ä¸ªä¼˜åŒ–å™¨
        self.optimizers = []
        question_type = self._infer_question_type(self.dataset)

        for i in range(self.env_num):
            optimizer = RLEnhancedOptimizer(
                dataset=self.dataset,
                question_type=question_type,
                opt_llm_config=self.opt_llm_config,
                exec_llm_config=self.exec_llm_config,
                operators=self.operators,
                sample=self.sample,
                check_convergence=False,
                optimized_path=self.workspace_path,
                initial_round=1,
                max_rounds=self.max_rounds,
                validation_rounds=self.validation_rounds,
                rl_policy=None,  # å°†ç”±è®­ç»ƒå™¨è®¾ç½®
                use_rl_guidance=True,
                rl_weight=self.rl_weight,
                shared_experience_pool=self.shared_experience_pool,
                state_manager=self.state_manager,
                enable_state_tracking=True
            )
            self.optimizers.append(optimizer)

        logger.info(f"[DeepWorkflowEnv] Created {self.env_num} RLEnhancedOptimizers")
        logger.info(f"[DeepWorkflowEnv] Shared pool size: {len(self.shared_experience_pool.experiences)}")

    def _infer_question_type(self, dataset: str) -> str:
        """æ¨æ–­é—®é¢˜ç±»å‹"""
        dataset_upper = dataset.upper()
        if dataset_upper in ["HUMANEVAL", "MBPP", "CODEEVAL"]:
            return "code"
        elif dataset_upper in ["AIME", "MATH", "GSM8K"]:
            return "math"
        else:
            return "qa"

    def _has_custom_evaluator(self, dataset: str) -> bool:
        """
        æ£€æŸ¥æ•°æ®é›†æ˜¯å¦æœ‰è‡ªå®šä¹‰evaluator

        Args:
            dataset: æ•°æ®é›†åç§°

        Returns:
            æ˜¯å¦æœ‰è‡ªå®šä¹‰evaluator
        """
        dataset_upper = dataset.upper()
        # ç›®å‰åªæœ‰AIMEæœ‰è‡ªå®šä¹‰evaluator
        return dataset_upper == "AIME"

    def _create_custom_evaluator(self, dataset: str, llm_config: Dict):
        """
        åˆ›å»ºæ•°æ®é›†ç‰¹å®šçš„è‡ªå®šä¹‰evaluator

        Args:
            dataset: æ•°æ®é›†åç§°
            llm_config: LLMé…ç½®

        Returns:
            è‡ªå®šä¹‰evaluatorå®ä¾‹
        """
        dataset_upper = dataset.upper()

        if dataset_upper == "AIME":
            from aime_evaluator import AIMEEvaluator
            return AIMEEvaluator(
                llm_config=llm_config,
                dataset_path="/content/agentflow/AFlow/data/AIME_2024.jsonl",
                sample_size=self.sample  # ä¼ é€’é…ç½®çš„ sample å‚æ•°
            )
        else:
            raise ValueError(f"No custom evaluator for dataset: {dataset}")

    def reset(self) -> Tuple[List[str], List[Dict]]:
        """
        é‡ç½®ç¯å¢ƒ

        Returns:
            observations: è§‚æµ‹åˆ—è¡¨
            info: ä¿¡æ¯å­—å…¸åˆ—è¡¨
        """
        self.current_round = 0

        observations = []
        info = []

        for i in range(self.env_num):
            # æ„é€ è§‚æµ‹ï¼šå‘Šè¯‰Qwenå½“å‰çŠ¶æ€
            obs = self._construct_observation(
                round_num=0,
                best_score=self.best_score,
                history_summary=self._get_history_summary()
            )
            observations.append(obs)

            info_dict = {
                'step': 0,
                'round': 0,
                'env_id': i,
                'best_score': self.best_score,
                'workflow_path': None
            }
            info.append(info_dict)

        logger.info(f"[DeepWorkflowEnv] Environment reset")
        return observations, info

    def step(self, actions: List[str]) -> Tuple[List[str], List[float], List[bool], List[Dict]]:
        """
        æ‰§è¡Œstep - è¿™é‡Œæ˜¯çœŸæ­£çš„workflowæ‰§è¡Œï¼

        æ ¹æ®æ¨¡å¼é€‰æ‹©ä¸åŒçš„æ‰§è¡Œè·¯å¾„ï¼š
        - Static Mode: è§£æQwenæè¿° â†’ ç”Ÿæˆä»£ç  â†’ æ‰§è¡Œæµ‹è¯•
        - Dynamic Mode: Qwenå»ºè®® â†’ RLEnhancedOptimizerä¼˜åŒ– â†’ è¿”å›åˆ†æ•°

        Args:
            actions: Qwenç”Ÿæˆçš„workflowæè¿°åˆ—è¡¨(Static) æˆ–ä¼˜åŒ–å»ºè®®(Dynamic)

        Returns:
            next_observations: ä¸‹ä¸€æ­¥è§‚æµ‹
            rewards: çœŸå®çš„workflowæ€§èƒ½åˆ†æ•°ï¼ˆpass@kï¼‰
            dones: æ˜¯å¦ç»“æŸ
            info: é¢å¤–ä¿¡æ¯
        """
        self.current_round += 1

        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ‰§è¡Œè·¯å¾„
        if self.use_dynamic_optimizer:
            return self._step_dynamic(actions)
        else:
            return self._step_static(actions)

    def _step_static(self, actions: List[str]) -> Tuple[List[str], List[float], List[bool], List[Dict]]:
        """
        é™æ€æ¨¡å¼çš„stepå®ç°
        """
        next_observations = []
        rewards = []
        dones = []
        info = []

        logger.info(f"[DeepWorkflowEnv] ===== Round {self.current_round} (STATIC) =====")
        logger.info(f"[DeepWorkflowEnv] Processing {len(actions)} workflow proposals...")

        for i, qwen_action in enumerate(actions):
            try:
                logger.info(f"[DeepWorkflowEnv] Env {i}: Processing Qwen output...")
                logger.info(f"[DeepWorkflowEnv] Env {i}: Action preview: {qwen_action[:200]}...")

                # 1. è§£æQwenè¾“å‡ºä¸ºworkflowè§„æ ¼ (ä¼ é€’datasetç±»å‹å’Œé‡‡æ ·æ•°)
                workflow_spec = self.workflow_parser.parse_qwen_output(
                    qwen_action,
                    dataset_type=self.dataset,
                    sample_count=self.workflow_sample_count
                )

                if workflow_spec is None:
                    logger.error(f"[DeepWorkflowEnv] Env {i}: Failed to parse Qwen output!")
                    rewards.append(0.0)
                    next_observations.append(self._construct_observation(
                        self.current_round, self.best_score, "Parse failed"
                    ))
                    dones.append(False)
                    info.append({'step': self.current_round, 'error': 'parse_failed'})
                    continue

                logger.info(f"[DeepWorkflowEnv] Env {i}: Parsed workflow:")
                logger.info(f"[DeepWorkflowEnv] Env {i}:   Modification: {workflow_spec.modification}")
                logger.info(f"[DeepWorkflowEnv] Env {i}:   Operators: {workflow_spec.operators}")

                # 2. ä¿å­˜workflowä»£ç åˆ°æ–‡ä»¶
                round_id = f"{self.current_round}_env{i}"
                workflow_path = self.workflow_parser.save_workflow_to_file(
                    workflow_spec,
                    round_id,
                    self.workspace_path
                )

                logger.info(f"[DeepWorkflowEnv] Env {i}: Workflow code saved to {workflow_path}")

                # 3. æ‰§è¡ŒçœŸå®çš„workflowæµ‹è¯•ï¼
                logger.info(f"[DeepWorkflowEnv] Env {i}: âš¡ EXECUTING REAL WORKFLOW TEST...")
                score = self._execute_workflow_test(round_id, workflow_path)

                self.total_tests_run += 1

                logger.info(f"[DeepWorkflowEnv] Env {i}: âœ… Real test score: {score:.4f}")
                logger.info(f"[DeepWorkflowEnv] Env {i}: This is a REAL pass@k score from HumanEval!")

                # 4. æ›´æ–°æœ€ä½³workflow
                if score > self.best_score:
                    logger.info(f"[DeepWorkflowEnv] Env {i}: ğŸ‰ NEW BEST SCORE! {self.best_score:.4f} -> {score:.4f}")
                    self.best_score = score
                    self.best_workflow = workflow_spec

                # 5. è®°å½•å†å²
                self.workflow_history.append({
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'workflow_spec': workflow_spec,
                    'workflow_path': workflow_path
                })

                # 6. è¿”å›çœŸå®åˆ†æ•°ä½œä¸ºreward
                reward = float(score)
                rewards.append(reward)

                # 7. æ„é€ ä¸‹ä¸€ä¸ªè§‚æµ‹
                next_obs = self._construct_observation(
                    round_num=self.current_round,
                    best_score=self.best_score,
                    history_summary=self._get_history_summary(),
                    last_score=score
                )
                next_observations.append(next_obs)

                # 8. åˆ¤æ–­æ˜¯å¦ç»“æŸ
                done = self.current_round >= self.max_rounds
                dones.append(done)

                # 9. Info
                info_dict = {
                    'step': self.current_round,
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'best_score': self.best_score,
                    'workflow_path': workflow_path,
                    'operators': workflow_spec.operators,
                    'modification': workflow_spec.modification,
                    'is_best': score == self.best_score
                }
                info.append(info_dict)

            except Exception as e:
                logger.error(f"[DeepWorkflowEnv] Env {i}: ERROR: {e}")
                import traceback
                traceback.print_exc()

                rewards.append(0.0)
                next_observations.append(self._construct_observation(
                    self.current_round, self.best_score, f"Error: {str(e)}"
                ))
                dones.append(False)
                info.append({'step': self.current_round, 'error': str(e)})

        avg_reward = np.mean(rewards) if rewards else 0.0
        logger.info(f"[DeepWorkflowEnv] Round {self.current_round} completed")
        logger.info(f"[DeepWorkflowEnv] Avg reward: {avg_reward:.4f}, Best so far: {self.best_score:.4f}")
        logger.info(f"[DeepWorkflowEnv] Total tests run: {self.total_tests_run}")

        return next_observations, rewards, dones, info

    def _step_dynamic(self, actions: List[str]) -> Tuple[List[str], List[float], List[bool], List[Dict]]:
        """
        åŠ¨æ€æ¨¡å¼çš„stepå®ç°
        ä½¿ç”¨ RLEnhancedOptimizer è¿›è¡Œ MCTS + RL ä¼˜åŒ–
        """
        next_observations = []
        rewards = []
        dones = []
        info = []

        logger.info(f"[DeepWorkflowEnv] ===== Round {self.current_round} (DYNAMIC) =====")
        logger.info(f"[DeepWorkflowEnv] Running {len(actions)} dynamic optimizations...")

        # å¹¶è¡Œè¿è¡Œæ‰€æœ‰ä¼˜åŒ–å™¨
        for i, (optimizer, action) in enumerate(zip(self.optimizers, actions)):
            try:
                logger.info(f"[DeepWorkflowEnv] Env {i}: Running RLEnhancedOptimizer...")
                logger.info(f"[DeepWorkflowEnv] Env {i}: Action hint: {action[:100]}...")

                # è¿è¡Œä¸€è½®ä¼˜åŒ–
                # RLEnhancedOptimizer ä¼šï¼š
                # 1. ç»“åˆ MCTS å’Œ RL é€‰æ‹©çˆ¶èŠ‚ç‚¹
                # 2. ä½¿ç”¨ LLM ç”Ÿæˆæ–° workflow
                # 3. åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
                # 4. æ›´æ–°å…±äº«ç»éªŒæ± 
                loop = asyncio.new_event_loop()
                asyncio.set_event_loop(loop)

                try:
                    score = loop.run_until_complete(optimizer._optimize_graph())
                except Exception as e:
                    logger.error(f"[DeepWorkflowEnv] Env {i}: Error in optimization: {e}")
                    import traceback
                    traceback.print_exc()
                    score = 0.0
                finally:
                    loop.close()

                logger.info(f"[DeepWorkflowEnv] Env {i}: âœ… Optimization score: {score:.4f}")

                # æ›´æ–°æœ€ä½³åˆ†æ•°
                if score > self.best_score:
                    logger.info(f"[DeepWorkflowEnv] Env {i}: ğŸ‰ NEW BEST SCORE! {self.best_score:.4f} -> {score:.4f}")
                    self.best_score = score
                    self.best_workflow = optimizer.graph

                # è®°å½•å†å²
                self.workflow_history.append({
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'workflow_type': 'dynamic'
                })

                # è¿”å›åˆ†æ•°ä½œä¸º reward
                reward = float(score)
                rewards.append(reward)

                # æ„é€ ä¸‹ä¸€ä¸ªè§‚æµ‹
                next_obs = self._construct_observation(
                    round_num=self.current_round,
                    best_score=self.best_score,
                    history_summary=self._get_history_summary(),
                    last_score=score
                )
                next_observations.append(next_obs)

                # åˆ¤æ–­æ˜¯å¦ç»“æŸ
                done = self.current_round >= self.max_rounds
                dones.append(done)

                # Info
                info_dict = {
                    'step': self.current_round,
                    'round': self.current_round,
                    'env_id': i,
                    'score': score,
                    'best_score': self.best_score,
                    'is_best': score == self.best_score,
                    'optimization_type': 'dynamic_rl_mcts'
                }
                info.append(info_dict)

            except Exception as e:
                logger.error(f"[DeepWorkflowEnv] Env {i}: ERROR: {e}")
                import traceback
                traceback.print_exc()

                rewards.append(0.0)
                next_observations.append(self._construct_observation(
                    self.current_round, self.best_score, f"Error: {str(e)}"
                ))
                dones.append(False)
                info.append({'step': self.current_round, 'error': str(e)})

        avg_reward = np.mean(rewards) if rewards else 0.0
        logger.info(f"[DeepWorkflowEnv] Round {self.current_round} completed")
        logger.info(f"[DeepWorkflowEnv] Avg reward: {avg_reward:.4f}, Best so far: {self.best_score:.4f}")
        logger.info(f"[DeepWorkflowEnv] Shared pool size: {len(self.shared_experience_pool.experiences)}")

        return next_observations, rewards, dones, info

    def _execute_workflow_test(self, round_id: str, workflow_path: str) -> float:
        """
        æ‰§è¡ŒçœŸå®çš„workflowæµ‹è¯•

        Args:
            round_id: round ID
            workflow_path: workflowä»£ç è·¯å¾„

        Returns:
            çœŸå®çš„pass@kåˆ†æ•°ï¼ˆ0.0-1.0ï¼‰
        """
        try:
            # å¯¼å…¥workflowæ¨¡å—
            round_dir = os.path.dirname(workflow_path)
            module_name = f"workspace.{self.dataset}.workflows_rl.round_{round_id}.graph"

            # åŠ¨æ€å¯¼å…¥
            spec = importlib.util.spec_from_file_location(module_name, workflow_path)
            module = importlib.util.module_from_spec(spec)
            sys.modules[module_name] = module
            spec.loader.exec_module(module)

            # è·å–Workflowç±»
            WorkflowClass = module.Workflow

            # åˆ›å»ºworkflowå®ä¾‹
            workflow = WorkflowClass(
                name=f"RL_Workflow_R{round_id}",
                llm_config=self.exec_llm_config,
                dataset=self.dataset
            )

            # ä½¿ç”¨evaluatoræ‰§è¡Œæµ‹è¯•
            # è¿™ä¼šçœŸæ­£è¿è¡Œæµ‹è¯•ä»»åŠ¡å¹¶è¿”å›pass@k
            logger.info(f"[DeepWorkflowEnv] Running real {self.dataset} test with sample={self.sample}...")

            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

            # å¦‚æœæ˜¯AIMEEvaluatorï¼Œéœ€è¦å…ˆåˆå§‹åŒ–
            if hasattr(self.evaluator, 'initialize') and self._has_custom_evaluator(self.dataset):
                if not self.evaluator.problems:  # åªåœ¨ç¬¬ä¸€æ¬¡åˆå§‹åŒ–
                    logger.info(f"[DeepWorkflowEnv] Initializing AIMEEvaluator...")
                    loop.run_until_complete(self.evaluator.initialize())
                    logger.info(f"[DeepWorkflowEnv] AIMEEvaluator initialized with {len(self.evaluator.problems)} problems")

            # æ‰§è¡Œè¯„ä¼°
            result = loop.run_until_complete(
                self.evaluator.evaluate_workflow(workflow)
            )

            loop.close()

            # resultæ˜¯è¯„ä¼°ç»“æœdictï¼Œæå–pass@kåˆ†æ•°
            score = result['pass_at_k'] if result and 'pass_at_k' in result else 0.0
            return float(score)

        except Exception as e:
            logger.error(f"[DeepWorkflowEnv] Workflow execution error: {e}")
            import traceback
            traceback.print_exc()
            return 0.0

    def _construct_observation(
        self,
        round_num: int,
        best_score: float,
        history_summary: str,
        last_score: Optional[float] = None
    ) -> str:
        """æ„é€ ç»™Qwençš„è§‚æµ‹"""
        obs = f"""Dataset: {self.dataset}
Task: Design and optimize agent workflow for code generation
Round: {round_num}/{self.max_rounds}

Current Best Score: {best_score:.4f}"""

        if last_score is not None:
            obs += f"\nLast Score: {last_score:.4f}"

        obs += f"""

Available Operators:
{', '.join(self.operators)}

{history_summary}

Your task: Generate a workflow description that will be converted to executable code and tested on real {self.dataset} problems. Focus on:
1. Which operators to use
2. How to combine them effectively
3. How to improve upon previous attempts
"""

        return obs

    def _get_history_summary(self) -> str:
        """è·å–å†å²workflowæ‘˜è¦"""
        if not self.workflow_history:
            return "History: No previous workflows yet."

        # å–æœ€è¿‘3ä¸ªworkflow
        recent = self.workflow_history[-3:]
        summary = "Recent Workflow Performance:\n"

        for item in recent:
            summary += f"  Round {item['round']} Env{item['env_id']}: "
            summary += f"Score={item['score']:.4f}, "
            summary += f"Operators={item['workflow_spec'].operators}\n"

        return summary

    def close(self):
        """å…³é—­ç¯å¢ƒ"""
        logger.info(f"[DeepWorkflowEnv] Environment closed")
        logger.info(f"[DeepWorkflowEnv] Total tests run: {self.total_tests_run}")
        logger.info(f"[DeepWorkflowEnv] Best score achieved: {self.best_score:.4f}")


def create_deep_workflow_env(dataset, opt_llm_config, exec_llm_config, **kwargs):
    """
    åˆ›å»ºæ·±åº¦workflowç¯å¢ƒçš„å·¥å‚å‡½æ•°

    æ”¯æŒä¸¤ç§æ¨¡å¼ï¼š
    - use_dynamic_optimizer=False (é»˜è®¤): é™æ€æ¨¡å¼ï¼Œä½¿ç”¨ WorkflowParser
    - use_dynamic_optimizer=True: åŠ¨æ€æ¨¡å¼ï¼Œä½¿ç”¨ RLEnhancedOptimizer
    """
    return DeepWorkflowEnv(
        dataset=dataset,
        opt_llm_config=opt_llm_config,
        exec_llm_config=exec_llm_config,
        operators=kwargs.get('operators', ['Custom', 'CustomCodeGenerate', 'ScEnsemble', 'Test']),
        env_num=kwargs.get('env_num', 2),
        sample=kwargs.get('sample', 3),
        max_rounds=kwargs.get('max_rounds', 10),
        workspace_path=kwargs.get('workspace_path'),
        workflow_sample_count=kwargs.get('workflow_sample_count'),
        use_dynamic_optimizer=kwargs.get('use_dynamic_optimizer', False),
        validation_rounds=kwargs.get('validation_rounds', 3),
        rl_weight=kwargs.get('rl_weight', 0.5)
    )


if __name__ == "__main__":
    # æµ‹è¯•ç¯å¢ƒ
    import yaml

    # åŠ è½½é…ç½®
    config_path = "deep_config_e2e.yaml"
    with open(config_path) as f:
        config = yaml.safe_load(f)

    env_config = config['environment']

    # åˆ›å»ºç¯å¢ƒ
    env = create_deep_workflow_env(
        dataset="HumanEval",
        opt_llm_config=env_config['opt_llm_config'],
        exec_llm_config=env_config['exec_llm_config'],
        operators=env_config['operators'],
        env_num=1,
        sample=2
    )

    # æµ‹è¯•
    obs, info = env.reset()
    print(f"Initial observation:\n{obs[0]}\n")

    # æ¨¡æ‹ŸQwenè¾“å‡º
    test_action = """
<workflow_modification>
Use ensemble approach to improve code quality
</workflow_modification>

<operators>
CustomCodeGenerate, ScEnsemble
</operators>

<workflow_steps>
1. Generate 3 candidate code solutions
2. Use ScEnsemble to select the best one
</workflow_steps>
"""

    next_obs, rewards, dones, info = env.step([test_action])
    print(f"Reward (real pass@k): {rewards[0]:.4f}")
    print(f"This is a REAL score from executing workflow on HumanEval!")
