# 优化的AIME训练配置 - Optimized AIME Training Config
# 解决问题：
# 1. 奖励始终为0 - 引入课程学习和奖励塑形
# 2. 学习效率低 - 增加样本效率和探索策略
# 3. 无检查点保存 - 改进保存策略

device: "cuda"

# 训练参数 - 1小时快速训练
total_epochs: 3
episodes_per_epoch: 3  # 减少episodes以加快速度
update_frequency: 1    # 每个episode后立即更新
eval_episodes: 1
save_frequency: 1      # 每个epoch都保存，以免丢失进度

output_dir: "/content/drive/MyDrive/agentflow/outputs/optimized_training"
experience_pool_size: 1000  # 减小经验池以节省内存

# 课程学习配置
curriculum:
  enable: true
  # 从简单问题开始，逐步增加难度
  stages:
    - epoch: 1        # 第1个epoch
      sample_size: 3   # 只用3个最简单的问题
      difficulty: "easy"
    - epoch: 2        # 第2个epoch
      sample_size: 4   # 增加到4个问题
      difficulty: "medium"
    - epoch: 3        # 第3个epoch
      sample_size: 6   # 6个问题
      difficulty: "all"

environment:
  train_datasets:
    - "AIME"
  test_datasets:
    - "AIME"
  data_path: "/content/agentflow/AFlow/data/AIME_2024.jsonl"

  opt_llm_config:
    model: "gpt-4o-mini"
    key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    temperature: 1.0  # 增加探索性

  exec_llm_config:
    model: "gpt-4o-mini"
    key: "${OPENAI_API_KEY}"
    base_url: "https://api.openai.com/v1"
    temperature: 0.7  # 执行时稍微保守

  operators:
    - "Custom"
    - "MathSolver"
    - "ScEnsemble"
    - "Test"
    - "Review"

  env_num: 1
  group_n: 1
  max_rounds: 5      # 减少轮数以加快训练
  validation_rounds: 3
  sample: 6          # 初始6个问题（会被curriculum覆盖）

# 改进的RL配置
rl:
  policy:
    model_path: "/root/models/Qwen2.5-7B-Instruct"
    use_lora: true
    lora_r: 16          # 较小的LoRA秩，训练更快
    lora_alpha: 32
    lora_dropout: 0.05  # 添加dropout防止过拟合
    value_head_dim: 1024 # 较小的value head，训练更快

  # 优化的学习率策略
  learning_rate: 0.0005  # 更高的学习率以加快学习
  lr_schedule: "cosine"  # 使用余弦退火
  warmup_steps: 50

  batch_size: 4          # 更小batch size以更频繁更新
  ppo_epochs: 3          # 减少PPO epochs
  ppo_clip: 0.2

  gamma: 0.99
  gae_lambda: 0.95
  value_coef: 1.0        # 增加value loss权重
  entropy_coef: 0.1      # 大幅增加熵奖励以鼓励探索
  gradient_clip: 0.5     # 更严格的梯度裁剪

  # 奖励塑形 - Reward Shaping
  reward_shaping:
    enable: true
    # 部分正确也给奖励
    partial_credit: true
    # 进步奖励：比上一次好就给奖励
    improvement_bonus: 0.1
    # 接近正确答案的奖励
    proximity_reward: true
    proximity_scale: 0.5
    # 基础奖励（避免全0）
    baseline_reward: 0.01

  # 改进的GiGPO配置
  gigpo:
    enable: true
    epsilon: 0.00001     # 增大epsilon避免数值问题
    step_advantage_w: 0.6
    mode: "mean_norm"
    enable_similarity: true
    similarity_thresh: 0.9
    workflow_similarity_thresh: 0.75

  # 探索策略
  exploration:
    enable: true
    # 温度退火：开始时高温度（多探索），后期低温度（多利用）
    initial_temperature: 1.2
    final_temperature: 0.7
    decay_epochs: 3

logging:
  level: "INFO"
  # 详细记录每个问题的得分变化
  log_problem_details: true
  # 保存workflow示例
  save_workflow_examples: true

debug:
  verbose_env: true
  # 保存失败的案例用于分析
  save_failed_cases: true
  # 每5轮打印详细统计
  detailed_stats_frequency: 5

# 早停策略
early_stopping:
  enable: false  # 禁用早停，确保完成所有epoch
  patience: 5  # 5个epoch没有improvement就停止
  min_improvement: 0.01

# 检查点策略
checkpoint:
  # 保存最好的3个模型
  keep_best_n: 3
  # 即使分数为0也要保存（记录探索过程）
  save_zero_score: true
  # 保存频率（每个epoch）
  save_every_epoch: true
