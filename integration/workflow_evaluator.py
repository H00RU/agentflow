"""
Workflow Evaluator - æ‰§è¡Œworkflowå¹¶è¯„ä¼°ç»“æœ
Real workflow execution and evaluation
æ”¯æŒå¤šç§æ•°æ®é›†ï¼šHumanEval, AIME, ç­‰
"""

import sys
import os
import asyncio
import time
import json
import re
from typing import Dict, Any, Optional, Tuple, List
from abc import ABC, abstractmethod
from pathlib import Path
import yaml

# Add AFlow to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'AFlow', 'scripts'))

try:
    from scripts.logs import logger
except:
    import logging
    logger = logging.getLogger(__name__)


# ===========================
# ç­”æ¡ˆéªŒè¯å™¨åŸºç±»å’Œå…·ä½“å®ç°
# ===========================

class AnswerValidator(ABC):
    """ç­”æ¡ˆéªŒè¯å™¨åŸºç±»"""

    @abstractmethod
    def validate(self, solution: str, problem: Dict, timeout: int = 10) -> Tuple[bool, str]:
        """
        éªŒè¯è§£ç­”æ˜¯å¦æ­£ç¡®

        Args:
            solution: LLMç”Ÿæˆçš„è§£ç­”ä»£ç /æ–‡æœ¬
            problem: åŒ…å«é—®é¢˜å’Œç­”æ¡ˆçš„å­—å…¸
            timeout: è¶…æ—¶æ—¶é—´(ç§’)

        Returns:
            (æ˜¯å¦é€šè¿‡, è¯¦ç»†è¯´æ˜)
        """
        pass


class CodeExecutionValidator(AnswerValidator):
    """
    ä»£ç æ‰§è¡ŒéªŒè¯å™¨ - ç”¨äºHumanEval
    æ‰§è¡ŒPythonä»£ç å¹¶æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹
    """

    def validate(self, solution: str, problem: Dict, timeout: int = 10) -> Tuple[bool, str]:
        """æ‰§è¡Œä»£ç å¹¶æ£€æŸ¥æµ‹è¯•"""
        if not solution or not problem.get('answer'):
            return False, "Missing solution or test code"

        test_code = problem['answer']
        entry_point = problem.get('entry_point', '')

        try:
            # åˆ›å»ºæ‰§è¡Œç¯å¢ƒ
            test_env = {}

            # æ‰§è¡Œsolutionä»£ç 
            exec(solution, test_env)

            # æ‰§è¡Œæµ‹è¯•ä»£ç 
            exec(test_code, test_env)

            # å¦‚æœæ²¡æœ‰å¼‚å¸¸ï¼Œè¯´æ˜é€šè¿‡
            return True, "Code execution passed all tests"

        except AssertionError as e:
            return False, f"Assertion failed: {str(e)}"
        except Exception as e:
            return False, f"Code execution error: {str(e)}"


class NumericComparisonValidator(AnswerValidator):
    """
    æ•°å€¼æ¯”è¾ƒéªŒè¯å™¨ - ç”¨äºAIMEç­‰æ•°å­¦é—®é¢˜
    ä»LLMè¾“å‡ºä¸­æå–æ•°å€¼ç­”æ¡ˆå¹¶ä¸æ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒ
    """

    def __init__(self, answer_pattern: str = r"<answer>(.*?)</answer>"):
        """
        åˆå§‹åŒ–

        Args:
            answer_pattern: ä»è¾“å‡ºä¸­æå–ç­”æ¡ˆçš„æ­£åˆ™è¡¨è¾¾å¼
        """
        self.answer_pattern = answer_pattern

    def validate(self, solution: str, problem: Dict, timeout: int = 10) -> Tuple[bool, str]:
        """æå–ç­”æ¡ˆå¹¶ä¸æ ‡å‡†ç­”æ¡ˆæ¯”è¾ƒ"""
        if not solution:
            return False, "No solution provided"

        correct_answer = problem.get('answer')
        if correct_answer is None:
            return False, "No correct answer provided"

        try:
            # æå–ç­”æ¡ˆ
            extracted_answer = self._extract_answer(solution)

            if extracted_answer is None:
                return False, f"Could not extract answer from solution. Expected: {correct_answer}"

            # æ¯”è¾ƒç­”æ¡ˆ
            is_correct, comparison = self._compare_answers(extracted_answer, correct_answer)

            if is_correct:
                return True, f"Correct answer: {extracted_answer} == {correct_answer}"
            else:
                return False, f"Wrong answer: {extracted_answer} != {correct_answer}. {comparison}"

        except Exception as e:
            return False, f"Answer validation error: {str(e)}"

    def _extract_answer(self, solution: str) -> Optional[str]:
        """
        ä»LLMè¾“å‡ºä¸­æå–ç­”æ¡ˆ

        å°è¯•å¤šç§æ–¹å¼ï¼š
        1. <answer>...</answer> æ ‡ç­¾
        2. æœ€åä¸€è¡Œçš„æ•°å­—
        3. æ•´ä¸ªè¾“å‡º
        """
        # æ–¹å¼1ï¼š<answer>æ ‡ç­¾
        matches = re.findall(self.answer_pattern, solution, re.DOTALL)
        if matches:
            return matches[-1].strip()

        # æ–¹å¼2ï¼šæœ€åä¸€è¡Œ
        lines = solution.strip().split('\n')
        for line in reversed(lines):
            line = line.strip()
            if line and any(c.isdigit() for c in line):
                # å°è¯•æå–æ•°å­—
                nums = re.findall(r'-?\d+(?:\.\d+)?', line)
                if nums:
                    return nums[-1]

        # æ–¹å¼3ï¼šæ•´ä¸ªè¾“å‡º
        nums = re.findall(r'-?\d+(?:\.\d+)?', solution)
        if nums:
            return nums[-1]

        return None

    def _compare_answers(self, extracted: str, correct: Any) -> Tuple[bool, str]:
        """
        æ¯”è¾ƒæå–çš„ç­”æ¡ˆå’Œæ­£ç¡®ç­”æ¡ˆ

        æ”¯æŒï¼š
        - æ•´æ•°æ¯”è¾ƒ
        - æµ®ç‚¹æ•°æ¯”è¾ƒ
        - åˆ†æ•°æ¯”è¾ƒï¼ˆ1/2 == 0.5ï¼‰
        - å­—ç¬¦ä¸²æ¯”è¾ƒ
        """
        try:
            # å°è¯•è½¬æ¢ä¸ºæ•°å­—
            extracted_num = self._parse_number(extracted)
            correct_num = self._parse_number(str(correct))

            if extracted_num is not None and correct_num is not None:
                # æ•°å€¼æ¯”è¾ƒ
                if isinstance(extracted_num, float) or isinstance(correct_num, float):
                    # æµ®ç‚¹æ•°æ¯”è¾ƒï¼ˆå…è®¸å°è¯¯å·®ï¼‰
                    tolerance = 1e-6
                    is_close = abs(extracted_num - correct_num) < tolerance
                    return is_close, f"Float comparison: {extracted_num} vs {correct_num}"
                else:
                    # æ•´æ•°æ¯”è¾ƒ
                    is_equal = int(extracted_num) == int(correct_num)
                    return is_equal, f"Integer comparison: {extracted_num} vs {correct_num}"

            # å­—ç¬¦ä¸²æ¯”è¾ƒ
            is_equal = str(extracted).strip().lower() == str(correct).strip().lower()
            return is_equal, f"String comparison: '{extracted}' vs '{correct}'"

        except Exception as e:
            return False, f"Comparison error: {str(e)}"

    def _parse_number(self, value: str) -> Optional[float]:
        """å°†å­—ç¬¦ä¸²è§£æä¸ºæ•°å­—ï¼ˆæ”¯æŒåˆ†æ•°ã€æµ®ç‚¹æ•°ç­‰ï¼‰"""
        try:
            # ç§»é™¤ç©ºæ ¼
            value = str(value).strip()

            # åˆ†æ•°å¤„ç†ï¼ša/b
            if '/' in value:
                parts = value.split('/')
                if len(parts) == 2:
                    numerator = float(parts[0])
                    denominator = float(parts[1])
                    return numerator / denominator

            # ç›´æ¥è½¬æ¢ä¸ºæµ®ç‚¹æ•°
            return float(value)
        except:
            return None


class LLMJudgeValidator(AnswerValidator):
    """
    LLMè¯„åˆ¤éªŒè¯å™¨ - ç”¨äºéœ€è¦è¯­ä¹‰ç†è§£çš„ä»»åŠ¡
    ä½¿ç”¨LLMæ¥è¯„åˆ¤ç­”æ¡ˆæ˜¯å¦æ­£ç¡®
    """

    def __init__(self, llm_engine=None):
        """
        åˆå§‹åŒ–

        Args:
            llm_engine: LLMå¼•æ“ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨GPT-4oï¼‰
        """
        self.llm_engine = llm_engine

    def validate(self, solution: str, problem: Dict, timeout: int = 10) -> Tuple[bool, str]:
        """ä½¿ç”¨LLMåˆ¤æ–­ç­”æ¡ˆ"""
        if not self.llm_engine:
            # å¦‚æœæ²¡æœ‰LLMå¼•æ“ï¼Œå›é€€åˆ°æ•°å€¼æ¯”è¾ƒæˆ–å­—ç¬¦ä¸²åŒ¹é…
            return self._fallback_validate(solution, problem)

        # TODO: å®ç°LLM Judgeé€»è¾‘
        # è¿™é‡Œå¯ä»¥è°ƒç”¨GPT-4Oè¿›è¡Œç­”æ¡ˆéªŒè¯
        return False, "LLM Judge not yet implemented"

    def _fallback_validate(self, solution: str, problem: Dict) -> Tuple[bool, str]:
        """å›é€€éªŒè¯æ–¹æ³•"""
        # ç®€å•çš„å­—ç¬¦ä¸²æ¯”è¾ƒ
        answer = problem.get('answer', '')
        if str(answer).strip().lower() in solution.lower():
            return True, "Answer found in solution"
        return False, "Answer not found in solution"


# ===========================
# ä¸»è¯„ä¼°å™¨ç±»
# ===========================

class WorkflowEvaluator:
    """
    Workflowè¯„ä¼°å™¨ - æ”¯æŒå¤šæ•°æ®é›†çš„ç»Ÿä¸€è¯„ä¼°æ¡†æ¶

    åŠŸèƒ½ï¼š
    1. åŠ è½½å¤šç§ç±»å‹çš„æ•°æ®é›†ï¼ˆHumanEval, AIME, ç­‰ï¼‰
    2. åœ¨æ•°æ®é›†ä¸Šè¿è¡Œworkflow
    3. ä½¿ç”¨æ•°æ®é›†ç‰¹å®šçš„éªŒè¯å™¨è¯„ä¼°ç»“æœ
    4. è®¡ç®—pass@kåˆ†æ•°
    5. è¿”å›è¯¦ç»†çš„æ€§èƒ½æŒ‡æ ‡
    """

    def __init__(
        self,
        dataset: str = "HumanEval",
        sample_size: int = 3,
        timeout_per_problem: int = 30,
        train_test_split: float = 0.8,
        llm_config: dict = None,
        config_file: str = None
    ):
        """
        åˆå§‹åŒ–evaluator

        Args:
            dataset: æ•°æ®é›†åç§° (HumanEval, AIME, AIME24ç­‰)
            sample_size: æµ‹è¯•æ ·æœ¬æ•°é‡
            timeout_per_problem: æ¯ä¸ªé—®é¢˜çš„è¶…æ—¶æ—¶é—´(ç§’)
            train_test_split: è®­ç»ƒé›†æ¯”ä¾‹ (é»˜è®¤0.8 = 80% train, 20% test)
            llm_config: LLMé…ç½®ï¼ˆç”¨äºåˆ›å»ºworkflowå®ä¾‹ï¼‰
            config_file: æ•°æ®é›†é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.dataset = dataset
        self.sample_size = sample_size
        self.timeout_per_problem = timeout_per_problem
        self.train_test_split = train_test_split
        self.llm_config = llm_config or {}

        # åŠ è½½æ•°æ®é›†é…ç½®
        self.config = self._load_dataset_config(config_file, dataset)

        # æ ¹æ®é…ç½®åŠ è½½æ•°æ®é›†
        self.problems = self._load_dataset()

        # åˆå§‹åŒ–ç­”æ¡ˆéªŒè¯å™¨
        self.validator = self._create_validator()

        logger.info(f"[WorkflowEvaluator] Initialized")
        logger.info(f"[WorkflowEvaluator] Dataset: {dataset}")
        logger.info(f"[WorkflowEvaluator] Sample size: {sample_size}")
        logger.info(f"[WorkflowEvaluator] Train/Test split: {int(train_test_split*100)}%/{int((1-train_test_split)*100)}%")
        logger.info(f"[WorkflowEvaluator] Evaluation method: {self.config.get('evaluation_method')}")
        logger.info(f"[WorkflowEvaluator] Loaded {len(self.problems)} problems")

    def _load_dataset_config(self, config_file: str = None, dataset: str = None) -> Dict:
        """åŠ è½½æ•°æ®é›†é…ç½®"""
        # ç¡®å®šé…ç½®æ–‡ä»¶è·¯å¾„
        if config_file is None:
            config_file = os.path.join(os.path.dirname(__file__), 'dataset_configs.yaml')

        # åŠ è½½é…ç½®
        try:
            with open(config_file, 'r') as f:
                all_configs = yaml.safe_load(f)
        except Exception as e:
            logger.warning(f"[WorkflowEvaluator] Could not load config file {config_file}: {e}")
            all_configs = {'datasets': {}}

        # è·å–è¯¥æ•°æ®é›†çš„é…ç½®
        dataset_config = all_configs.get('datasets', {}).get(dataset, {})

        if not dataset_config:
            logger.warning(f"[WorkflowEvaluator] No config found for dataset '{dataset}', using defaults")
            return {
                'evaluation_method': 'code_execution',
                'sample_size': 10,
                'train_test_split': 0.8
            }

        return dataset_config

    def _load_dataset(self) -> Dict:
        """æ ¹æ®é…ç½®åŠ è½½æ•°æ®é›†"""
        data_path = self.config.get('data_path', '')

        # æ•°æ®è·¯å¾„æ˜¯ç›¸å¯¹äºagentflowç›®å½•çš„
        if not os.path.isabs(data_path):
            data_path = os.path.join(
                os.path.dirname(__file__),
                '..',
                data_path
            )

        data_path = os.path.abspath(data_path)

        logger.info(f"[WorkflowEvaluator] Loading dataset from: {data_path}")

        if not os.path.exists(data_path):
            logger.warning(f"[WorkflowEvaluator] Data file not found: {data_path}")
            return self._create_dummy_problems()

        try:
            if data_path.endswith('.json'):
                return self._load_json_dataset(data_path)
            elif data_path.endswith('.jsonl'):
                return self._load_jsonl_dataset(data_path)
            else:
                logger.warning(f"[WorkflowEvaluator] Unknown file format: {data_path}")
                return self._create_dummy_problems()

        except Exception as e:
            logger.error(f"[WorkflowEvaluator] Error loading dataset: {e}")
            return self._create_dummy_problems()

    def _load_json_dataset(self, data_path: str) -> Dict:
        """åŠ è½½JSONæ ¼å¼çš„æ•°æ®é›†"""
        with open(data_path, 'r') as f:
            items = json.load(f)

        problems = {}

        # å¤„ç†æ•°ç»„æ ¼å¼ï¼ˆå¦‚AIME24ï¼‰
        if isinstance(items, list):
            for item in items:
                # ä½¿ç”¨idxæˆ–pidä½œä¸ºkey
                problem_id = f"{self.dataset}/{item.get('idx', item.get('pid', len(problems)))}"
                problems[problem_id] = self._standardize_problem(item)

        # å¤„ç†å­—å…¸æ ¼å¼
        elif isinstance(items, dict):
            for key, item in items.items():
                problem_id = f"{self.dataset}/{key}"
                problems[problem_id] = self._standardize_problem(item)

        logger.info(f"[WorkflowEvaluator] âœ… Loaded {len(problems)} problems")
        return problems

    def _load_jsonl_dataset(self, data_path: str) -> Dict:
        """åŠ è½½JSONLæ ¼å¼çš„æ•°æ®é›†"""
        problems = {}

        with open(data_path, 'r') as f:
            for line in f:
                item = json.loads(line)
                problem_id = item.get('task_id', f"{self.dataset}/{len(problems)}")
                problems[problem_id] = self._standardize_problem(item)

        logger.info(f"[WorkflowEvaluator] âœ… Loaded {len(problems)} problems")
        return problems

    def _standardize_problem(self, item: Dict) -> Dict:
        """
        å°†åŸå§‹é—®é¢˜è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼

        æ ‡å‡†æ ¼å¼ï¼š
        {
            'question': é—®é¢˜æ–‡æœ¬ï¼ˆç»™LLMçš„è¾“å…¥ï¼‰,
            'answer': æ ‡å‡†ç­”æ¡ˆï¼ˆç”¨äºéªŒè¯ï¼‰,
            'entry_point': å‡½æ•°å…¥å£ç‚¹ï¼ˆå¯é€‰ï¼‰,
            'original': åŸå§‹æ•°æ®ï¼ˆä¿ç•™å¤‡ç”¨ï¼‰
        }
        """
        fields = self.config.get('problem_fields', {})

        # è·å–é—®é¢˜å­—æ®µå
        question_field = fields.get('question', 'question')
        answer_field = fields.get('answer', 'answer')
        entry_point_field = fields.get('entry_point', 'entry_point')

        standardized = {
            'question': item.get(question_field, item.get('question', '')),
            'answer': item.get(answer_field, item.get('answer')),
            'entry_point': item.get(entry_point_field, item.get('entry_point', 'solve')),
            'original': item  # ä¿ç•™åŸå§‹æ•°æ®
        }

        return standardized

    def _create_dummy_problems(self) -> Dict:
        """åˆ›å»ºdummyé—®é¢˜ç”¨äºæµ‹è¯•"""
        evaluation_method = self.config.get('evaluation_method', 'code_execution')

        if evaluation_method == 'numeric_comparison':
            # AIMEé£æ ¼çš„dummyé—®é¢˜
            return {
                f'{self.dataset}/0': {
                    'question': 'Find the sum of all positive integers less than 1000 that are divisible by 3 or 5.',
                    'answer': 233168,
                    'entry_point': 'solve',
                    'original': {}
                }
            }
        else:
            # HumanEvalé£æ ¼çš„dummyé—®é¢˜
            return {
                f'{self.dataset}/0': {
                    'question': 'def has_close_elements(numbers: List[float], threshold: float) -> bool:\n    """ Check if in given list of numbers..."""',
                    'answer': 'def check(candidate):\n    assert candidate([1.0, 2.0, 3.9], 0.3) == True',
                    'entry_point': 'has_close_elements',
                    'original': {}
                }
            }

    def _create_validator(self) -> AnswerValidator:
        """æ ¹æ®è¯„ä¼°æ–¹æ³•åˆ›å»ºç­”æ¡ˆéªŒè¯å™¨"""
        evaluation_method = self.config.get('evaluation_method', 'code_execution')

        if evaluation_method == 'code_execution':
            return CodeExecutionValidator()

        elif evaluation_method == 'numeric_comparison':
            pattern = self.config.get('answer_extraction_pattern', r'<answer>(.*?)</answer>')
            return NumericComparisonValidator(answer_pattern=pattern)

        elif evaluation_method == 'llm_judge':
            return LLMJudgeValidator(llm_engine=None)

        else:
            logger.warning(f"[WorkflowEvaluator] Unknown evaluation method: {evaluation_method}")
            return CodeExecutionValidator()  # é»˜è®¤ä½¿ç”¨ä»£ç æ‰§è¡Œ

    async def evaluate_workflow(
        self,
        workflow,
        num_problems: Optional[int] = None,
        use_test_set: bool = False,
        random_sample: bool = True
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°workflowæ€§èƒ½

        Args:
            workflow: Workflowå®ä¾‹
            num_problems: æµ‹è¯•çš„é—®é¢˜æ•°é‡ï¼ˆNone=ä½¿ç”¨sample_sizeï¼‰
            use_test_set: æ˜¯å¦ä½¿ç”¨æµ‹è¯•é›†ï¼ˆTrue=æµ‹è¯•é›†ï¼ŒFalse=è®­ç»ƒé›†ï¼‰
            random_sample: æ˜¯å¦éšæœºé‡‡æ ·ï¼ˆTrue=éšæœºï¼ŒFalse=å›ºå®šå‰Nä¸ªï¼‰

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«ï¼š
            - pass_at_k: pass@kåˆ†æ•°
            - num_passed: é€šè¿‡çš„é—®é¢˜æ•°
            - num_total: æ€»é—®é¢˜æ•°
            - avg_time: å¹³å‡æ—¶é—´
            - details: è¯¦ç»†ç»“æœ
        """
        start_time = time.time()

        # é€‰æ‹©æµ‹è¯•é—®é¢˜
        if num_problems is None:
            num_problems = min(self.sample_size, len(self.problems))

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        all_problem_ids = list(self.problems.keys())
        train_size = int(len(all_problem_ids) * self.train_test_split)
        train_ids = all_problem_ids[:train_size]
        test_ids = all_problem_ids[train_size:]

        # é€‰æ‹©æ•°æ®é›†
        if use_test_set:
            available_ids = test_ids
            logger.info(f"[WorkflowEvaluator] ğŸ“Š Using TEST set ({len(test_ids)} problems available)")
        else:
            available_ids = train_ids
            logger.info(f"[WorkflowEvaluator] ğŸ“š Using TRAIN set ({len(train_ids)} problems available)")

        # é‡‡æ ·é—®é¢˜
        if random_sample and num_problems < len(available_ids):
            import random
            problem_ids = random.sample(available_ids, min(num_problems, len(available_ids)))
            logger.info(f"[WorkflowEvaluator] ğŸ² Randomly sampled {len(problem_ids)} problems")
        else:
            problem_ids = available_ids[:min(num_problems, len(available_ids))]
            logger.info(f"[WorkflowEvaluator] ğŸ“‹ Using first {len(problem_ids)} problems")

        logger.info(f"[WorkflowEvaluator] Testing workflow on {len(problem_ids)} problems...")

        results = []
        num_passed = 0

        for i, task_id in enumerate(problem_ids):
            problem = self.problems[task_id]

            logger.info(f"[WorkflowEvaluator] [{i+1}/{len(problem_ids)}] Testing {task_id}...")

            try:
                # è¿è¡Œworkflow
                problem_start = time.time()

                solution, cost = await asyncio.wait_for(
                    workflow(
                        problem=problem['question'],
                        entry_point=problem['entry_point']
                    ),
                    timeout=self.timeout_per_problem
                )

                problem_time = time.time() - problem_start

                # éªŒè¯solution
                passed, validation_msg = self.validator.validate(solution, problem)

                if passed:
                    num_passed += 1
                    logger.info(f"[WorkflowEvaluator] {task_id}: âœ… PASSED - {validation_msg}")
                else:
                    logger.info(f"[WorkflowEvaluator] {task_id}: âŒ FAILED - {validation_msg}")

                results.append({
                    'task_id': task_id,
                    'passed': passed,
                    'time': problem_time,
                    'cost': cost,
                    'solution_length': len(solution) if solution else 0,
                    'validation_message': validation_msg
                })

            except asyncio.TimeoutError:
                logger.error(f"[WorkflowEvaluator] {task_id}: â±ï¸ TIMEOUT")
                results.append({
                    'task_id': task_id,
                    'passed': False,
                    'time': self.timeout_per_problem,
                    'error': 'timeout'
                })

            except Exception as e:
                logger.error(f"[WorkflowEvaluator] {task_id}: âŒ ERROR: {e}")
                results.append({
                    'task_id': task_id,
                    'passed': False,
                    'time': 0,
                    'error': str(e)
                })

        total_time = time.time() - start_time

        # è®¡ç®—pass@k
        pass_at_k = num_passed / len(problem_ids) if problem_ids else 0.0
        avg_time = total_time / len(problem_ids) if problem_ids else 0.0

        evaluation_result = {
            'pass_at_k': pass_at_k,
            'pass_at_1': pass_at_k,
            'num_passed': num_passed,
            'num_total': len(problem_ids),
            'avg_time': avg_time,
            'total_time': total_time,
            'details': results,
            'dataset': self.dataset,
            'evaluation_method': self.config.get('evaluation_method')
        }

        logger.info(f"[WorkflowEvaluator] ===== EVALUATION COMPLETE =====")
        logger.info(f"[WorkflowEvaluator] Pass@{len(problem_ids)}: {pass_at_k:.4f} ({num_passed}/{len(problem_ids)})")
        logger.info(f"[WorkflowEvaluator] Avg time: {avg_time:.2f}s")
        logger.info(f"[WorkflowEvaluator] Total time: {total_time:.2f}s")

        return evaluation_result

    def quick_test(self, workflow, num_problems: int = 1) -> float:
        """
        å¿«é€Ÿæµ‹è¯•workflowï¼ˆç”¨äºRLè®­ç»ƒï¼‰

        Args:
            workflow: Workflowå®ä¾‹
            num_problems: æµ‹è¯•é—®é¢˜æ•°ï¼ˆé»˜è®¤1ï¼Œæ›´å¿«ï¼‰

        Returns:
            pass@kåˆ†æ•°
        """
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)

        try:
            result = loop.run_until_complete(
                self.evaluate_workflow(workflow, num_problems=num_problems)
            )
            return result['pass_at_k']

        finally:
            loop.close()


# ===========================
# å•ä¾‹å’Œå·¥å…·å‡½æ•°
# ===========================

_global_evaluator = None


def get_evaluator(dataset: str = "HumanEval", sample_size: int = 3):
    """è·å–å…¨å±€evaluatorå•ä¾‹"""
    global _global_evaluator
    if _global_evaluator is None:
        _global_evaluator = WorkflowEvaluator(
            dataset=dataset,
            sample_size=sample_size
        )
    return _global_evaluator


if __name__ == "__main__":
    # æµ‹è¯•evaluator
    print("Testing WorkflowEvaluator...")

    evaluator = WorkflowEvaluator(dataset="AIME", sample_size=1)

    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•workflow
    class DummyWorkflow:
        async def __call__(self, problem, entry_point):
            # è¿”å›ä¸€ä¸ªç®€å•çš„solution
            solution = f"""
<answer>233168</answer>
"""
            return solution, 0.01

    workflow = DummyWorkflow()

    # æµ‹è¯•
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    result = loop.run_until_complete(
        evaluator.evaluate_workflow(workflow, num_problems=1)
    )

    print(f"\nTest result:")
    print(f"Pass@K: {result['pass_at_k']:.4f}")
    print(f"Passed: {result['num_passed']}/{result['num_total']}")

    loop.close()
